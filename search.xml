<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[教你用neo4j做lookalike（下）]]></title>
    <url>%2Fposts%2Fneo4j-lookalike-part2%2F</url>
    <content type="text"><![CDATA[在上文中，我们已经做好了基本的数据预处理，并生成了供Neo4J导入的csv格式数据。在这节中，让我们把数据导入Neo4J，开始我们的图数据库之旅。 Neo4J数据导入这里假设大家对neo4j已经有了基本的了解。不清楚的同学可以参考下我之前写的文章手把手教你快速入门知识图谱 - Neo4J教程。 一般来说，neo4j的数据导入有以下几种方式： CYPHER CREATE。这种方式可以实时插入，但是速度和吞吐量都很慢，只适合实验场景。 Java或Python API。这种方式适合小数据量的实时插入，实现一定程度上的增删改查。 CYPHER LOAD CSV。这种方式速度就好很多，适合中等数据量的批量插入。 Neo4j-import。这种方式速度是最快的，但缺点是插入的时候必须停止Neo4j，且只能生成新的数据库，因此只适合大数据量的初始化。 我们这里采用的是CYPHER LOAD CSV的方式。在上一章节中，已经生成好了相应的CSV，下面就是调用CYPHER语句将数据导入。这里有2个小trick： 我们是先导入节点node，然后再导入关系relationship。导入关系的逻辑，是先查找出相应的节点，再建立关系，因此为了查询速度的优化，我们要给节点的查找字段添加索引，如：12CREATE INDEX ON :Person(cust_num)CREATE INDEX ON :Phone(phone_num) 另外，索引建立好后，记得运行:schema确保索引已生效。 如果搭建neo4j的服务器性能有限，要避免jvm虚拟机在批量插入时内存不足，需要使用以下语句进行批量的事务提交1USING PERIODIC COMMIT 1000 节点导入接下来就是LOAD客户节点了，注意这里有个操作，是将CSV中格式为STRING的embedding list转化为FLOAT list。 1234LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/nodes.csv&quot;AS lineWITH line, split(substring(line.embedding, 1, length(line.embedding)-2), &quot;,&quot;) as emsCREATE (:Person &#123;cust_num:line.cust_num, cust_name:line.cust_name, longitude:TOFLOAT(line.longitude), latitude:TOFLOAT(line.latitude), embedding:[item in ems | TOFLOAT(item)]&#125;) 导入成功后，在neo4j里面查看一下，Person节点信息如下： 然后是手机号节点及城市节点： 123456789LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/phone_nodes.csv&quot;AS lineWITH lineCREATE (:Phone &#123;phone_num:line.phone_num&#125;)LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/city_nodes.csv&quot;AS lineWITH lineCREATE (:City &#123;city_cd:line.city&#125;) 关系导入首先是用户拥有手机号的关系： 123456LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/has_phone_relations.csv&quot;AS lineWITH lineMATCH (f:Person &#123;cust_num: line.cust_num&#125;), (t:Phone &#123;phone_num: line.phone_num&#125;)MERGE (f)-[r:HAS_PHONE]-&gt;(t)RETURN count(r) 关系建立好后如图： 然后以此类推，分别是用户联系人的关系和用户所在城市的关系： 12345678910111213LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/has_contact_relations.csv&quot;AS lineWITH lineMATCH (f:Person &#123;cust_num: line.cust_num&#125;), (t:Phone &#123;phone_num: line.phone_num&#125;)MERGE (f)-[r:HAS_CONTACT]-&gt;(t)RETURN count(r)LOAD CSV WITH HEADERS FROM &quot;file:///var/lib/neo4j/import/in_city_relations.csv&quot;AS lineWITH lineMATCH (f:Person &#123;cust_num: line.cust_num&#125;), (t:City &#123;city_cd: line.city&#125;)MERGE (f)-[r:IN_CITY]-&gt;(t)RETURN count(r) lookalike实现图谱建立好以后，就可以开始应用了。我们来看看在Neo4J中是如何优雅的实现的。 联系人扩散联系人扩散的逻辑，就是根据种子用户，找到这些用户的联系人（父母、配偶、朋友等）。在图数据库里，可以非常自然的用以下语句实现： 123MATCH (n:Person &#123;cust_num: &quot;3814&quot;&#125;)-[r:HAS_CONTACT]-&gt;(p:Phone)&lt;-[q:HAS_PHONE]-(m:Person)WHERE n&lt;&gt;mRETURN n, r, p, q, m 查询结果如下： 查询结果非常快，响应速度是毫秒级，而且这个速度跟数据量级基本关系不大。这就是图数据库相比传统的RDMS关系型数据库的优势。注意这里是2级关系的查询，关系级数越大，优势越明显。这是因为RDMS中对于这种关系的查找，需要全量数据的join，这是非常消耗计算资源的。而在图数据库中，关系作为“第一公民”，只会在需要查询的节点上做计算，不涉及join，因此速度非常快，且基本跟数据量无关。 地理位置扩散地理位置扩散，难度会稍大一些了。业务逻辑是找到跟种子用户地理位置相近的用户，也就是说，根据GPS经纬度，找到某用户物理距离最近的一群用户。这就涉及了所有数据计算中的“噩梦”–笛卡尔积。 通过图数据库，我们可以在一定程度上预先减少笛卡尔积的运算量。还记得我们有个“IN_CITY”的关系么？我们可以预先查询出用户所在城市的其他用户，只在这些用户之间比较距离即可。如果数据做得细，可以增加城区、街道、甚至小区，这样计算量就更小了。 另外，根据两点的经纬度算物理距离，neo4j中也提供直接的函数，不用自己来写了。 123456match (n1:Person &#123;cust_num:&quot;200&quot;&#125;)-[r:IN_CITY]-&gt;(p:City)&lt;-[q:IN_CITY]-(n2:Person)where n1&lt;&gt;n2with n1, n2, point(&#123;longitude:n1.longitude, latitude:n1.latitude&#125;) as p1,point(&#123;longitude:n2.longitude, latitude:n2.latitude&#125;) as p2return n1.cust_num, distance(p1,p2)/1000 as dist, n2.cust_numorder by dist asc 同样查询速度非常快，并且返回了客户号为200的用户，跟他距离最近的用户号及公里数。 用户行为扩散这种扩散方式更加的“AI”。也是各个大厂的lookalike算法体现功力的地方。这里有两个问题，一是用户embedding如何生成，二是如何查询。 对于用户embedding的生成，我们在上文中采用的传统的机器学习的方式，使用已经做好的用户标签库来处理。在这里neo4j的帮助不大，我们不再多说。 在生成好的用户embedding上查询相近似的用户，这是典型的nearest neighbor场景，同样存在着笛卡尔积的噩梦。由于计算量非常的大，我们不推荐在每次查询时实时计算，而是预先在初始化的时候，把每个用户距离最近的TOP20计算好，动态生成固定关系，然后在服务的时候，只做一次查询操作即可。 注意这里使用到了neo4j的图算法库algo，下载好相应的jar包如neo4j-graph-algorithms-3.5.13.0-standalone.jar放到neo4j安装目录中的plugins文件夹，然后在conf文件中写入： 1dbms.security.procedures.unrestricted=algo.* 接下来，就可以用如下CYPHER语句来生成关系了： 12345678910match (p:Person)with &#123;item:id(p), weights:p.embedding&#125; as userDatawith collect(userData) as dataCALL algo.similarity.euclidean(data, &#123; topK:20, write:true, showComputations: true, writeRelationshipType: &quot;SIMILAR20&quot;&#125;)YIELD nodes, similarityPairs, computationsRETURN nodes, apoc.number.format(similarityPairs) as similarityPairs,apoc.number.format(computations) as computations 这里我们采用了欧氏距离，并且为了演示计算的复杂度，我们把计算量computations也打印出来。 可以看到，我们的大致有9550个用户，neo4j安装在笔记本电脑的docker中，这条CYPHER语句运行了33秒，生成了19万条关系，计算量是惊人的4500万！ 初始化完成以后，就可以开始查询了： 12match (from:Person &#123;cust_num:&quot;200&quot;&#125;)-[r:SIMILAR20]-&gt;(to:Person)return distinct from, r, to 从结果可以看到，迅速返回了客户号为“200”的“客户余”跟他最相似的客户节点，以及这些客户之间的相似度分数。 批量扩散上面几种演示，基本涵盖了常见的扩散方式。实际操作的过程中，需要根据场景、需求，设计好图模型，就能使用neo4j轻松实现lookalike。不过，还有一点就是，上面为了演示方便，都只查询了一个种子用户的情况，正常情况下，都是一批种子，如何实现呢？其实，用neo4j的unwind语句就可以了。比如我们要批量查询5个用户的联系人扩散： 1234UNWIND [&quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;] as xMATCH (n:Person &#123;cust_num: x&#125;)-[r:HAS_CONTACT]-&gt;(p:Phone)&lt;-[q:HAS_PHONE]-(m:Person)WHERE n&lt;&gt;mRETURN m.cust_num unwind语句可以理解为编程语言中的循环，把一个列表中的值依次代入到后续的语句中执行，就可以高效地得到批量的结果。 总结本文以一个实际的案例，分别从联系人扩散、地理位置扩散、用户行为扩散三方面演示了使用neo4j图数据库去做lookalike用户扩散的方法。当然，这里只是demo，在实际的生产环境中，还需要考虑很多工程化的问题，比如： 用户数据的更新如何导入，是T+1全量，还是实时增量？如果每日T+1，如果解决neo4j批量数据删除与重建？ 当用户数据量达到一定程度，在地理位置扩散中，即使预先选定了城市，数据量也非常大，如何提升查询效率？ 同样当用户量很大时，预先计算用户行为向量相似度的笛卡尔积也非常的大，计算量呈几何级数增长，非常容易内存OOM导致计算失败，如何解决？ 这些具体问题，我们在后续的文章中再来探讨。]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Neo4J</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你用neo4j做lookalike（上）]]></title>
    <url>%2Fposts%2Fneo4j-lookalike-part1%2F</url>
    <content type="text"><![CDATA[起源精准营销一直是大数据技术商业落地的重要场景之一。通过对用户的数据分析，找到精准的受众群来营销，可以在节省营销预算的前提下大幅提升转化率，使得数据产生价值。 在精准营销的各种技术中，有个很时髦的技术叫做“lookalike”。 什么是“lookalike”呢？ Lookalike技术基于种子用户画像和社交关系链寻找相似的受众，即在大量用户群中选择一组特定的种子（即有转化行为的）受众，包括但不局限于点击、下载、安装、激活，然后根据实际需求，筛选、识别、拓展更多相似受众，进一步引发更大客户量级的倍增。 我们先来看看lookalike之前，我们怎么来通过大数据精准营销。 首先，假定通过前期的大数据建设，我们已经有了比较完备的用户画像标签库，包括用户性别、年龄、偏好等等。我们的业务人员根据业务经验，直接可以根据标签规则来筛选出用户群。举个例子，对于护肤品来说，用户群一般就是“18-40岁，女性，美妆偏好”等。筛选出来的潜在用户群，可以直接进行广告推送等触达，转化率肯定比广撒网更高，而且花费的成本还低。 但是，这种方式仍然有局限性。其一，如果业务人员无法准确定位人群特征怎么办？其二，如果需要对100w人投放，标签筛选只选出了30w人怎么办？ 2012年，Facebook在广告领域开始应用定制化受众（Facebook CustomAudiences）功能，这个功能可以让企业不用再选择标签，包括用户的基本信息、兴趣等，企业需要做的只是上传一批目前已有的用户或者感兴趣的一批用户，剩下的工作，就由受众发现功能来完成。 像Facebook这样通过一群已有的用户发现并扩展出其他用户的算法就叫Lookalike。各个大厂纷纷在这个功能上发力，典型的包括腾讯在微信端的广告推荐的应用等。具体实现的技术也有多种，有基于规则定义的，有机器学习、深度学习来建模的，也有通过社交网络的。可以看到，lookalike是基于用户画像之上的、更直接的一种大数据应用。 今天，我们来介绍一种很优雅的方式：通过neo4j图数据库来实现lookalike。 思路lookalike，即相似人群扩展。基本原理就是根据各种维度，找到跟种子用户群相似的人群。本文从最常见的几种场景入手，将维度大致分为基于联系人扩散、基于地理位置扩散、基于行为扩散。当然，实际生产中的扩散规则需要根据业务场景来仔细设计。 用户数据，特别是社交网络上的用户数据，天然是图结构。节点是用户，边就是好友或联系人。使用图数据库，在选定某用户的情况下，根据社交关系来扩散，可以取得非常高效的查询效果。同样，我们把用户与用户之间的地理距离做成边、用户与用户之间的余弦相似度做成边，那么相似的查询接口，可以把这三种扩散关系快速得到。 我们的图数据schema可以见下图： Person表示用户实体，Phone表示手机号码实体，City表示城市实体。 用户和手机号之间有HAS_PHONE和HAS_CONTACT关系，分别表示拥有手机号和联系人号码。 用户和城市之间有IN_CITY关系，表示所在城市。 用户和用户之间有SIMILAR关系，表示余弦相似。 数据我们的数据直接来源于用户画像，一般是存储在ElasticSearch或者HBase之上，以用户id为rowkey，每个标签为一个特征，标签值为特征值。标签包括静态属性如年龄、性别、手机号、联系人等，动态属性如gps位置、最近的点击行为、下载行为、购买行为等等。 我们将原始数据导入Pandas，得到一个shape为（9550, 428）的DataFrame，即共有9550个用户，428个特征。 接下来是数据处理。处理的目的，是生成实体和关系的csv文件，用于neo4j的数据导入。 城市实体城市实体很简单，取出DataFrame中的所有城市字段，去重即可。 1234567all_city = set(df['City_Cd'].values)with open('city_nodes.csv', 'w', newline='', encoding='utf-8') as f: headers = ['city'] f_csv = csv.writer(f) f_csv.writerow(headers) for item in all_city: f_csv.writerow([item]) 手机号实体手机号码分别存储在用户手机号及联系人手机号字段里，也是全部取出再去重： 12all_phone_num = set(np.concatenate((df['Mobile_Num'].values, df['Contact_Tel'].values))) 用户实体用户实体比较复杂。首先用户id，用户姓名，gps经纬度都可以直接得到。要做用户之间的余弦相似度，得把用户行为表示成一定维度的向量才行。这里就牵涉了基本的机器学习数据预处理操作。 首先我们将所需要的用户行为相关的特征字段分为连续数值型(numerical)和类别型(categorical)。 12data_action_categorical = df[action_columns_string]data_action_numerical = df[action_columns_float] 针对categorical类型，需要做OneHot处理，我们直接用sklearn的api。 12action_oh_enc = OneHotEncoder(handle_unknown='ignore')data_action_categorical_onehot = action_oh_enc.fit_transform(data_action_categorical) 针对numerical类型，需要做归一化，这样才能把每个维度的特征放在统一尺度来进行相似度运算。 12mms_action = MinMaxScaler()data_action_numerical = mms_action.fit_transform(data_action_numerical) 然后就可以把它们拼接在一起。由于sklearn的OneHot生成的是sparse矩阵，需要用到scipy.sparse的hstack。 12df_action = hstack([data_action_numerical, data_action_categorical_onehot])# df_action.shape (9550, 99) 这样，每个用户的行为数据被处理成一个99维的向量，可以进行余弦相似度运算了。在实际工作中，考虑到计算资源和效率的问题，可以使用PCA等算法将这个向量继续降维，具体可参考使用numpy来理解PCA和SVD。 然后就是用户实体csv的创建了。除了常规的属性，我们把用户行为向量作为embedding属性存储。 12345678with open('nodes.csv', 'w', newline='', encoding='utf-8') as f: headers = ['tree_id', 'cust_num', 'cust_name', 'longitude', 'latitude', 'embedding'] f_csv = csv.writer(f) f_csv.writerow(headers) index = 0 for item in df.itertuples(): f_csv.writerow([item[0], item[1], item[2], item[3], item[4], list(df_action[index])]) index += 1 可以大致看一下生成的csv的情况： HAS_PHONE关系这个比较简单，我们将csv分为两列，第一列为用户id，第二列为手机号 123456with open('has_phone_relations.csv', 'w', newline='', encoding='utf-8') as f: headers = ['cust_num', 'phone_num'] f_csv = csv.writer(f) f_csv.writerow(headers) for item in df.itertuples(): f_csv.writerow([item[1], item[3]]) 同理，我们可以得到HAS_CONTACT关系、IN_CITY关系，分别得到相应的CSV。 至此，数据处理已基本完毕，剩下的任务就是把csv数据导入neo4j，开始我们的图数据之旅，且听下回分解。]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Neo4J</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[且看他深度学习兵分三路，直指推荐系统]]></title>
    <url>%2Fposts%2Fdeep-learning-in-recommendation-system%2F</url>
    <content type="text"><![CDATA[起源自互联网兴起以来，人们逐渐从信息匮乏走向了信息过载的时代。从成千上万的网页、新闻，到各种音乐、视频，如何样从如此繁多的信息中找到我们想要的，就成了互联网时代的一个非常重要的课题。 以Yahoo为代表的分类目录网站，首先给出了解决方案。他们将网站信息系统地分类整理，人们可以根据需求逐级浏览。但是，这种方案只能覆盖少量的热门网站。 紧接着，以Google为代表搜索引擎，在对信息进行组织和处理后，为用户提供了检索服务，解决了长尾需求的问题。但是，如果用户无法找到准确需求的描述呢？比如说，我今天想看一部电影，哪个电影好看，我也不知道。这种场景下，搜索引擎也解决不了。 这时，以Amazon为代表的公司开始了推荐系统的研究。从用户行为中找到用户的个性化需求，再将信息准确的推荐给需要它的用户。作为一个桥梁，帮助用户发现对自己有价值的信息，让信息能够展现在对它感兴趣的用户面前，这就是推荐系统的使命。 推荐系统是一个非常庞大的话题。今天我们不展开来讲。我们今天的话题是在当前学术界和工业界，伴随着深度学习在计算机视觉领域和自然语言处理领域的攻城拔寨，他如何去攻占下一个目标 – 推荐系统（更准确的说 – 推荐算法）？ 主流大数据技术和AI算法是当今推荐系统的基础。通过大数据平台，可以实现对海量用户行为的准确采集和存储；通过数据挖掘算法，可以通过用户行为挖掘分析用户的个性与偏好，形成现实世界用户的数学建模 – 用户画像；通过NLP及音频/视频理解算法，可以形成准确描述物品的物品画像。在这些基础之上，形成了当前工业界主流的推荐系统架构。以视频推荐的场景为例，如下图 ： 抛开工程方面的系统架构，单独来看推荐算法相关，可以分为两个部分：召回算法和排序算法。 召回算法的作用，是从百万级的视频库里，通过一些相对简单粗暴的算法，快速筛选用户可能感兴趣的视频，把物品量级降到数百个，这么做的目的主要是从节省计算成本的角度考虑。常用的召回算法举例： 基于用户画像的算法 基于内容相似度的算法 协同过滤算法 当然，还可以在召回阶段，根据运营人员的人工经验，进行人为的视频召回。 由于在召回阶段把物品的量级降到了百级，那么就可以通过排序算法来做精细化处理了。排序算法的目的，就是在这召回的几百个视频里，根据用户的点击率预估、观看时长预估等目标，把这些视频按照得分来排序。排在最前面的，就是用户最喜欢或者最有可能去看的，在推荐的时候，就优先展示出来。这一块由于计算压力相对较小，是各种算法大放异彩的时候。从经典的逻辑回归LR、到Facebook的Xgboost+LR、再到Google一鸣惊人的Wide &amp; Deep、乃至后面的DeepFM、DCN等等，也是深度学习预先攻占的领地。 再此以后，深度学习如何继续深入推荐系统领域，去取得在计算机视觉、NLP类似的“吊打传统”的成就呢？ 兵分三路注意力机制Attention，中文翻译为注意力机制，源自于NLP的机器翻译领域。 上图为机器翻译中的典型场景 – Sequence to Sequence，主流做法是用Encoder – Decoder的架构来做。当人类需要把”How are you”这句英语翻译成中文时，我们并不是一个单词一个单词的翻译，而是翻译的过程中不断参考上下文，不同的上下文，对于当前翻译的文字作用不同，在数学里面，就是权重不同。 2017年，Google的论文《Attention is All You Need》横空出世，以Attention为主要结构的Transformer，结束了NLP领域中RNN的统治，再次基础上各种NLP模型如Bert等，也不断刷新着NLP领域的记录。同样，在语音和视频等有连续序列的场景中，Attention也是各种广泛应用，人们纷纷说：真香。 那么推荐系统呢？其实推荐系统也是典型的序列场景。以网络购物为例，一个用户在半年之内购买的商品，就是一个序列。根据之前的序列，预测他之后会买的商品，就是推荐。因此，在推荐系统中应用Attention，再自然不过。 来自阿里妈妈的盖坤团队，就是这么做的。他们提出了DIN – Deep Interest Network。 他们认为，用户的兴趣是非常多样的，而当前的候选商品，仅仅只是跟用户的一部分兴趣有关。在考虑排序的时候，针对每一个候选商品，分别计算用户以往购买物品的Attention权重，然后把历史记录通过Attention权重计算embedding。 比如上图中，一位女士购买过鞋、包、外套、杯子等，且外套买了很多，当她购买新的冬季外套时，这个兴趣只跟她之前买过的冬季外套相关，跟她买过的鞋、包、杯子等行为无关。通过Attention来使得当前候选物品局部激活历史兴趣，实现Local Activation，同时，权重的多样性也能实现用户兴趣多样性的表达。 图神经网络如果说注意力机制是深度学习在推荐系统中的正常发展的话，基于知识图谱的图神经网络，就是一路奇兵了。 知识谱图，Knowledge Graph，即大规模语义网络。它把现实世界抽象成一张网络，其中富含实体（entity）、概念（concept）、及其之间的各种语义关系（relationship）。 知识图谱早在20世纪60年代就开始有人研究，当时称为知识工程，是符号主义的主要观点（深度学习代表着连接主义）。2012年，Google收购了Metaweb公司，正式发布了知识图谱，主要应用于搜索领域。典型的应用场景，你在搜索框里搜“姚明的身高”，回复的答案直接是229cm，而不是以往的文本字符串匹配。 知识图谱代表着认知智能，它使得AI可以理解到具体文本背后代表的知识。比如用户喜欢C罗，那么我们就可以推理出用户喜欢足球，那么给用户推荐皇马的比赛、世界杯葡萄牙队的比赛就最自然不过了。 深度学习攻占知识谱图领域的姿势，是“图表示学习”（Graph Representation Learning），即图神经网络。通过图表示学习，将知识图谱中的节点、边等都表示为特征向量，这些向量都蕴含着节点间的知识信息，然后各种知识的推理过程，就可以通过数学的矩阵运算来得到。 那么，深度学习通过图神经网络来做推荐系统，同样水到渠成。 我们以新闻推荐为例。当用户看过一堆新闻后，首先通过NLP技术识别出新闻中的实体，如Trump、Tim Cook等，然后根据已有的知识图谱，可以得到这些实体附近“一跳”（甚至多跳）的实体，通过图嵌入（Graph Embedding）得到这些这些实体的特征向量。有了这些向量，不管是通过传统的向量相似度、还是作为一个特征放到排序算法中，就有很多方式来做推荐了。 知识图谱是我认为目前非常有潜力的一个方向，它代表着AI从感知智能向认知智能的发展，同时，它非常好的解决了当前机器学习解释性弱的问题。 强化学习推荐系统一直存在一个痛点，即EE问题（Explore &amp; Exploit）。Explore即探索，指在未知的特征空间的采样；Exploit即利用，指利用探索的结果更有效的预测。 举个例子，通常的推荐算法是基于用户的历史行为的。你经常听周杰伦的歌，我就给你推周杰伦，或者类似港台的流行歌曲。久而久之，用户会进入到“信息茧房”之中，永远都是听类似的，没有“惊喜”，这就是Exploit；而为了解决这个问题，我们必须进行Explore以提升推荐系统的多样性，去尝试推荐一些用户没有听过的歌，虽然这些歌用户可能会不喜欢。 在机器学习中，这是做不到的。机器学习的根基是基于已有样本的归纳和拟合，而对于未知特征空间，我们是无法采样并构造训练集的。 传统推荐系统中，有人在尝试Bandit算法，即多臂老虎机问题（MAB，Multi-armed bandit problem）。而这个算法，就是当前大热的强化学习的简化版。 强化学习，即Reinforcement Learning，又称为增强学习。这是区别于常见的监督学习、无监督学习之外的，另一条强劲的AI分支。大家应该都知道Google的AlphaGo大战李世石、柯洁，这背后强化学习功不可没。 强化学习的中心思想，就是让智能体（Agent）在环境（Environment）里面学习。每个行动（Action）都会对应各自的奖励（reward），智能体通过分析数据来学习，怎样的情况下应该做怎样的事情。这种方式是非常自然的，比如有个小孩子，第一次看到了火，走到火边感受到了火的温暖，他知道火是个好东西（reward+1），当他尝试去摸火时，又被火烫到（reward-1）。最后他得出了结论，离火稍远一点就行，不能接近。AI通过强化学习，跟人类自身去学习大自然是及其相似的。 传统的强化学习代表算法包括Q-Learning等，在深度学习时代，通过深度神经网络来近似Q值，提出了Deep Q-Learning等。 同样以新闻推荐的场景举例： 用户和新闻构成了环境， 推荐算法是智能体。当智能体收到一个新闻请求后，先根据用户特征和新闻特征，随机推荐K个新闻。用户通过点击或不点击来作为反馈，而推荐算法可以实时或者定时，根据每个推荐及反馈，来更新策略，最终使得算法点击率达到最优。 通过深度强化学习，可以不断的进行Explore探索，并采用不断优化的方式来Exploit，实现准确性和多样性的平衡，一定程度上解决了EE问题；另外，它还能解决推荐系统冷启动的问题，即当系统刚上线，或者新用户到来，没有那么多的数据来进行传统的监督学习，那么通过强化学习来实现过程中的逐步优化，就是很好的方式了。 总结本文对推荐系统的起源、概念、以及主流的推荐架构进行了介绍，重点分享了在深度学习时代，学术界及工业界去做推荐算法的新思路。正如最近加入阿里的贾扬清大神所说，如果深度学习攻破了当前占据了当前互联网大多数资源和算力的推荐系统、广告领域，就像它在计算机视觉取得的成就一样，那么带来的价值将会是非常巨大的。我相信，这只是时间问题。]]></content>
      <categories>
        <category>Recommendation System</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>DIN</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何理解CNN中的平移不变性？]]></title>
    <url>%2Fposts%2Fcnn-shift-invariant%2F</url>
    <content type="text"><![CDATA[前言深度学习在近两年大红大紫，主要功臣莫过于在计算机视觉computer vision领域的突破。而这里面，深度学习相对传统方式的最大优势，是通过神经网络结构实现特征的自动提取：直接把原始图像像素点输入，网络可以自动提取出有利于判断的特征。 实现这一切的关键，就是CNN（Convolutional Neural Networks）卷积神经网络。今天我们不展开来讲CNN，而是去理解CNN里面的一个重要性质 - 平移不变性。 认识大神正文之前还是要先拜一拜CNN之父 - Yann LeCun，深度学习三巨头之一、笑傲AI界的男人、拥有中文译名最多的法国人😄 CNN原理简介 卷积神经网络（Convolutional Neural Networks, CNN）是一类包含 卷积 计算且具有深度结构的 前馈神经网络 （Feedforward Neural Networks），是 深度学习 （deep learning）的代表算法之一 [1-2] 。卷积神经网络具有 表征学习 （representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为”平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）“ 卷积首先我们来看什么是卷积。我们将问题简化，假设原始图像像素矩阵如下： 即高度为4，宽度为4，通道数是1。 假设filter是这样： 即shape=(2, 2, 1) 我们将filter放在原始图像的左上角，做卷积操作（矩阵各个元素相乘，最后相加），结果为1x0+1x0+1x0+1x1=1。 以此类推，如果我们将filter沿着原始图像从左到右，从上到下，依次扫描，每一次扫描都做一次卷积操作，则可以得到一个新的矩阵。 还是延续上面的例子，我们将扫描操作简化，no padding, no strides，可以得到： 学过《信号与系统》的同学肯定能记得老师在课上经常讲的一句话： 时域卷积等于频域相乘时域卷积等于频域相乘时域卷积等于频域相乘 简单来说，CNN中的卷积操作，可以理解为多个不同类型的滤波器的滑动窗口在图像全图滑动，最后得到滤波后的结果。而这多个滤波器不是预先定义的，是网络通过不断的训练学习到的。 MaxPoolingCNN中还有一个非常重要的操作，叫MaxPooling，中文翻译为最大池化，即取局部接受域中值最大的点。 这个操作很简单，还是继续上面的例子，我们把得到的结果做一次3x3的MaxPooling，即找出3x3格子里的最大值，即为3。当然通常情况下也是需要做滑动操作的。 平移不变性好了，了解了CNN的基本概念，我们可以来看平移不变性了。 首先解释下平移不变性 : 假设我们需要做图像识别，判断一张图片里面是猫还是狗： 那么不论这张图片里的猫或狗位于图像的哪个位置（上面还是下面，左边还是右边），我们训练好的神经网络都能够把它识别出来。而CNN恰好就具有这样的性质。 如何理解呢？ 我们将上面的例子扩展一下，这里有三幅图片 这里的三角形分别位于中间，左边和上边。 我们还是以相同的2x2 filter做卷积 可以得到滤波后的结果 然后，再同时做3x3的MaxPooling，结果如何？ 可以看到，不管三角形位于何处，最后都能得到相同的结果，而这个结果就是神经网络最后做图像识别的判断依据。 我们可以说，CNN的平移不变性，很大程度需要归功于MaxPooling，因为它是取的某个区域内的极大值。 最后平移、旋转、scaling作为图像识别的三大经典问题，一直都是人们研究的热点。大神Yann LeCun陪伴Hinton三十年磨一剑，最终使用深度学习CNN很好的解决了这些问题，使得跌宕了几十年的深度神经网络重生，最终问鼎世界。 “深度神经网络既漂亮，又光亮透明”，LeCun说。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机眼中的数学，跟你所学的不一样]]></title>
    <url>%2Fposts%2Fmath-in-computer%2F</url>
    <content type="text"><![CDATA[Photo by Helloquence on Unsplash 引子先来一道简单的小学数学题：已知函数f(x)定义如下 若x=0.1，将f(x)的结果一直代入f(x)，迭代n次，结果是怎样？ 我相信，任何具有小学数学水平的同学都能解出这道数学题：0.2, 0.4, 0.8, 0.6, 0.2, 0.4, 0.8, 0.6, 0.2, 0.4, 0.8, 0.6, 无限循环 但是我要告诉你，目前人工智能AI的基础-计算机，却无法解出这道题。你信不？ 实验你肯定要说了，不可能！计算机不就是做各种计算的么，各种高等数学微分积分都不在话下，怎么可能这道题都解不出？ OK, talk is cheap, show me the code. 你熟练的拿出MacBook Pro，啪啪啪写下如下代码, 一气呵成： 12345678910def f(x): if x &lt;= 0.5: return 2 * x if x &gt; 0.5: return 2*x - 1x = 0.1for i in range(80): print(x) x = f(x) 伴随着你嘴角上扬的微笑，运行了这段python代码，结果是： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879800.10.20.40.80.60000000000000010.200000000000000180.400000000000000360.80000000000000070.60000000000000140.200000000000002840.40000000000000570.80000000000001140.60000000000002270.200000000000045470.400000000000090950.80000000000018190.60000000000036380.20000000000072760.40000000000145520.80000000000291040.60000000000582080.200000000011641530.400000000023283060.80000000004656610.60000000009313230.200000000186264510.400000000372529030.80000000074505810.60000000149011610.200000002980232240.40000000596046450.8000000119209290.60000002384185790.200000047683715820.400000095367431640.80000019073486330.60000038146972660.200000762939453120.400001525878906250.80000305175781250.6000061035156250.200012207031250.40002441406250.8000488281250.600097656250.20019531250.4003906250.800781250.60156250.2031250.406250.81250.6250.250.51.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.0 傻眼了没？ 为什么你肯定不敢相信自己的眼睛。这在数学上是无法解释，说不通的啊？ 其实，现实就是这么残酷，这是现代计算机的能力和限制。最根本的原因：计算机眼中的数学，跟我们学的数学不一样。 我们学习的数学中的数，是无限的、是连续的；而计算机能够存储和表示的数，是有限的、离散的。 上图就是计算机中存储浮点数的方式，可以看到包括了符号（sign）、数值（mantissa）和指数（exponent）三部分。大家都知道，计算机中的小数常用float来表示，而float32、float64分别代表这这个小数的精度。因此，1.2-1在计算机中并非精确的0.2。不信你再试试: 121.2 - 10.19999999999999996 以有限表示无限、以离散表示连续，难免会造成误差。当我们计算的次数非常多时，误差不断累积，结果就会南辕北辙。 怎么办在普通的计算机程序里面，这种情况还好，因为很多都是整数计算，即使是小数计算，只要计算次数不多，四舍五入以后也看不出问题。 但是，在机器学习和深度学习中，就不是这样了。比如深度学习的基础 - 随机梯度下降，就是各种小数、微分、不断迭代。 因此，为了解决这些问题，有个专门的学科 - 数值方法。 数值方法（numerical method，也称计算方法、数值分析等）是利用计算机进行数值计算来解决数学问题的方法，其研究内容包括数值方法的理论、分析、构造及算法等。很多科学与工程问题都可归结为数学问题，而数值方法对很多基本数学问题建立了数值计算的解决 办法，例如线性代数方程组的求解、多项式插值、微积分和常微分方程的数值解法等等。 通过数值方法，我们可以保证在一定程度上的“数值稳定性 (Numerical Stability)”，即将误差控制在一定范围内。具体操作包括避免除以趋近于零的数，或者避免非常多的小数连续相乘。 在机器学习中，为什么非常多的高手喜欢用log，就有因为log操作能够有这方面的功效。 我们都知道在数学上log有个非常重要的性质: 即可以把乘法变成加法。试想下，如果x和y都是比较小的小数，连续相乘势必会非常影响计算精度，而转化成加法后，就可以把误差控制在一定范围内。 所以说，要搞好AI，不光数学功底要过硬，计算机工程也同等重要。 总结我们平时做算法工作时，在数学上推导出了公式，把公式转化成计算机程序，切记要有数值方法的思维，采用各种操作来减少误差，因为，计算机眼中的数学跟你眼中的不一样。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用numpy来理解PCA和SVD]]></title>
    <url>%2Fposts%2Fnumpy-pca-svd%2F</url>
    <content type="text"><![CDATA[前言线性代数是高等数学里面非常重要的一门课程，可惜在学校的时候是一种自底向上的学习方式，并不知道学出来有什么用，以致彻底沦为应试教育。后来在工作中接触了机器学习，才真正看到了“数学之美”，才发现线性代数是多么的优雅多么的有用。 今天我们来看看下线性代数中非常重要的一个知识点奇异值分解SVD Singular value decomposition。SVD在数据科学当中非常有用，其常见的应用包括： 自然语言处理中的Latent Semantic Analysis 推荐系统中的Collaborative Filtering 降维常用套路Principal Component Analysis LSA已经在前文中有所讲解，CF的话后面在推荐系统的专题中来写，今天主要聊聊PCA，以及SVD在PCA中的重要作用。同样延续我们“手撕”的传统，使用numpy来理解其中的原理。 PCAPrincipal component analysis即主成分分析，是机器学习中一种非常常用的降维方式。其发源也是源自于早期的计算机处理能力有限，当数据样本的维度很高时，预先去除掉数据中的一些冗余信息和噪声（降维），使得数据变得更加简单高效，节省时间和成本。 在深度学习时代，更强调的是原始数据的直接输入，再通过神经网络来做降维工作，最典型是场景就是计算机视觉，直接输入原始图片像素信息，通过CNN卷积层、MaxPooling层来进行降维。因此PCA逐渐开始淡出人们的视线，通常是作为一种数据可视化的手段（二维图表无法展示多维的数据样本）。 其实，在深度学习目前尚未全面攻克的结构化数据领域，PCA仍然有较多的用，其数据处理的思路依然值得我们去学习揣摩。 PCA正常解法PCA算法的本质，其实就是找到一些投影方向，使得数据在这些投影方向上的方差最大，且这些投影方向是相互正交的。找到新的正交基后，计算原始数据在这些正交基上投影的方差，方差越大，就说明对应正交基上包含了更多的信息量。 关于原始数据的方差，最好的一个工具就是协方差矩阵了。协方差矩阵的特征值越大，对应的方差也就越大，在对应的特征向量上投影的信息量就越大。因此，我们如果将小特征值对应方向的数据删除，就可以达到降维的目的。因此，在数学上，我们可以把问题转化为求原始数据的协方差矩阵，然后计算协方差矩阵的特征值与特征向量。 对于广大程序员来说，学习机器学习最重要的一个坎还是数学。很多实际的代码其实是公式推导后的结果的代码实现，如果没有理清公式推导的过程，那么最后肯定是一头雾水。所以，克服心中的恐惧，翻出压在箱底的《线性代数》，我们上。 首先，求原始数据X的协方差矩阵C，将原始矩阵中心化后，做如下操作 接着，由于协方差矩阵C是方阵，就可以通过特征分解的方式来求C的特征值和特征向量。 最后，选择最大的k个特征值进行保留，求X的k阶PCA（X右乘k阶特征向量） 用SVD来解PCA根据上面的推导，我们已经可以对矩阵X做PCA了。同学们可能要问了，这跟SVD有什么关系呢？ 工程化思维强的同学应该已经想到了，这种纯数学的解法，在实际工程实践中有以下问题： 在数据量很大时，把原始矩阵进行转置求协方差矩阵，然后再进行特征值分解是一个非常慢的过程。 稳定性问题。可以看到X转置乘以X，如果矩阵有非常小的数，很容易在平方中丢失。 工业界中，还是“唯快不破，唯稳不破”。我们知道，奇异值分解相对特征分解，有个很大的优势就是不要求原始矩阵是方阵。这非常符合现实生活中的数据。因此，有大神想到，是否可以用svd来解PCA？推导如下： 我们根据协方差矩阵的公式，把X按照奇异值分解展开，注意后面应用到了一个酋矩阵(unitary)的特性： 看到最后的结果，是否跟上面的 很像？没错。协方差矩阵C的特征值和X的奇异值有以下关系 而C的特征向量即为X的SVD分解后的V向量, 则参考PCA正常解法，X的k阶PCA即为_X右乘k阶V向量_。因此这种方式求PCA，只需要把原始矩阵做一次SVD分解即可，不用转置，不用求协方差矩阵。事实上，在Scikit Learn等机器学习框架中，就是用的SVD来做PCA。 用numpy来验证numpy原始解法求PCA接下来，我们用numpy来验证这种思路。首先是PCA的标准解法： 随机模拟一个原始数据矩阵，5个样本，3个特征： 123456789import numpy as npX = np.random.rand(5,3)'''array([[0.86568791, 0.73022945, 0.17982869], [0.07201287, 0.99358411, 0.84389196], [0.61267696, 0.08867997, 0.11770573], [0.16898969, 0.3093472 , 0.9010064 ], [0.43840269, 0.97250927, 0.64897872]])''' 将矩阵中心化，即减去均值： 12345678910X_new = X - np.mean(X, axis=0)'''array([[ 0.43413389, 0.11135945, -0.35845361], [-0.35954115, 0.37471411, 0.30560966], [ 0.18112294, -0.53019003, -0.42057657], [-0.26256433, -0.3095228 , 0.3627241 ], [ 0.00684866, 0.35363927, 0.11069642]])'''# 确保结果正确，即转换后均值为0np.allclose(X_new.mean(axis=0), np.zeros(X_new.shape[1])) 求X_new的协方差矩阵C 1234567C = np.dot(X_new.T, X_new) / (X_new.shape[0] - 1)'''Carray([[ 0.10488363, -0.02467955, -0.10903811], [-0.02467955, 0.16369454, 0.05611495], [-0.10903811, 0.05611495, 0.13564834]])''' 求C的特征值和特征向量，这里用的是numpy的特征分解函数 123456789eig_vals, eig_vecs = np.linalg.eig(C)'''eig_valsarray([0.26474535, 0.00779743, 0.13168373])eig_vecsarray([[-0.53801107, 0.72610049, -0.42816139], [ 0.50584138, -0.12820944, -0.85304562], [ 0.67429117, 0.67552974, 0.29831358]])''' 求X的PCA结果，就是X右乘k阶特征向量。这里k还是取的3。 12345678X_pca = np.dot(X_new, eig_vecs)'''array([[-0.41894072, 0.05880142, -0.38780564], [ 0.58905292, -0.10265648, -0.07453908], [-0.64922927, -0.08462316, 0.24926273], [ 0.22927473, 0.09406657, 0.4846625 ], [ 0.24984234, 0.03441165, -0.27158052]])''' numpy的SVD求PCA首先，直接求X_new的SVD，同样使用numpy的函数 12345678910111213141516# 注意这里的Vh其实是公式中的VTU, Sigma, Vh = np.linalg.svd(X_new, full_matrices=False, compute_uv=True)'''Uarray([[-0.40710685, 0.53434046, 0.33295236, 0.59843699, -0.28241844], [ 0.57241386, 0.10270414, -0.58127362, 0.5471169 , -0.15677466], [-0.63089041, -0.34344824, -0.47916326, 0.32579597, 0.38507162], [ 0.22279838, -0.66779531, 0.53263483, 0.46842625, 0.03587876], [ 0.24278501, 0.37419895, 0.19484969, 0.13026931, 0.86376738]])Sigmaarray([1.02906823, 0.72576506, 0.17660612])Vharray([[-0.53801107, 0.50584138, 0.67429117], [ 0.42816139, 0.85304562, -0.29831358], [ 0.72610049, -0.12820944, 0.67552974]])''' 我们来根据上面的公式，确认下eig_vals和S的关系，注意在numpy的实现中，特征值和奇异值的排序是不同的 1234567np.allclose(eig_vals, np.square(S) / (X_new.shape[0] - 1))'''eig_valsarray([0.26474535, 0.00779743, 0.13168373])np.square(S) / (X_new.shape[0] - 1)array([0.26474535, 0.13168373, 0.00779743])''' 从结果看出，确实跟公式是一致的。 接下来用SVD求PCA就简单了，直接右乘V即可。 1234567891011# 注意Vh是公式中的VT，因此V=Vh.TX_pca_svd = np.dot(X_new, Vh.T)# X_pca_svd = np.dot(U, np.diag(Sigma))'''X_pca_svdarray([[-0.41894072, 0.38780564, 0.05880142], [ 0.58905292, 0.07453908, -0.10265648], [-0.64922927, -0.24926273, -0.08462316], [ 0.22927473, -0.4846625 , 0.09406657], [ 0.24984234, 0.27158052, 0.03441165]])''' 求出结果后，正当我们信心满满的对比一下X_pca和X_pca_svd,以为大功告成打完收工时，却发现二者是不一致的。WTF？ 结果分析仔细研究下X_pca和X_pca_svd的结果，可以看出，排除特征值和奇异值的排序导致的列向量顺序不同外，部分列向量的绝对值相同但正负不同。 问题出在哪里？我们搬出Scikit Learn，再来算一次PCA： 1234567891011from sklearn.decomposition import PCApca = PCA(3)pca.fit_transform(X) # sklearn自动处理去均值化'''array([[ 0.41894072, -0.38780564, -0.05880142], [-0.58905292, -0.07453908, 0.10265648], [ 0.64922927, 0.24926273, 0.08462316], [-0.22927473, 0.4846625 , -0.09406657], [-0.24984234, -0.27158052, -0.03441165]])''' 嗯，也是绝对值相同但正负不同。都说Scikit Learn的PCA就是SVD做的，难道是骗人的？ 好在代码不会骗人，我们直接翻出源码。 通过研究Scikit Learn的源码svd_flip@scikit-learn/extmath.py找到了答案： SVD奇异值分解的结果是唯一的，但是分解出来的U矩阵和V矩阵的正负可以不是唯一，只要保证它们乘起来是一致的就行。因此，sklearn为了保证svd分解结果的一致性，它们的方案是：保证U矩阵的每一行(u_i)中，绝对值最大的元素一定是正数，否则将u_i转成-u_i,并将相应的v_i转成-v_i已保证结果的一致。 这又是数学与工程的问题了。在数学上，几种结果都是正确的。但是在工程上，有个很重要的特性叫幂等性(Idempotence)。 Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. 这是源自于HTTP规范中的一个概念，可以引申至各种分布式服务的设计当中，即：高质量的服务，一次请求和多次请求，其副作用（结果）应当是一致的。Scikit Learn正是通过svd_flip这个函数，把一个数学上并不幂等的操作，转化成了幂等的服务，其设计之讲究可见一斑。 总结本文通过公式推导和numpy代码实战，展示了PCA的正常解法，以及工业界常用的SVD解法，并最后引申至数学和实现的一些探讨。“part-science, part-art”，这就是我最喜爱的机器学习之道]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>SVD</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras中return_sequences和return_state有什么用？]]></title>
    <url>%2Fposts%2Fkeras-return-sequences-return-state%2F</url>
    <content type="text"><![CDATA[Photo by Jon Tyson on Unsplash 前言CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发展的。 今天，我们以LSTM为例，来谈一个RNN中的一个具体的问题。我们知道，在Keras的LSTM实现中，有两个参数return_sequences和return_state。这两个参数的实际意义是什么？在什么场景下会用到呢？ PS：Keras是我最喜爱的深度学习框架了，其API的设计非常精妙和优雅，François Chollet是不愧是大师中的大师。相比传统的Tensorflow和PyTorch，Keras的API才是真正的“Deep Learning for Human”。另外，看到Tensorflow 2.0也开始以tf.keras作为第一公民，我非常欣慰。关于我对这几个框架的理解，后面再以专题文章和大家分享。 LSTM介绍 Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997) , and were refined and popularized by many people in following work.They work tremendously well on a large variety of problems, and are now widely used. LSTM是为了解决普通RNN网络在实际实践中出现的“梯度消失”等问题而出现的。这里我们略过里面的细节，重点看看单个LSTM cell的输入输出情况。从上图可以看出，单个LSTM cell其实有2个输出的，一个是h(t)，一个是c(t)。这里的h(t)称为hidden state，c(t)称为cell state。这个命名其实我认为是不太好的。熟悉全连接神经网络的同学，一定会把h(t)跟hidden layer相混淆。其实，这个h(t)才是LSTM的真正output，c(t)才是LSTM的内部”隐藏”状态。 我们进一步把LSTM网络展开来看。每一个时间节点timestep，输入一个x(t)，cell里面的c(t)做一次更新，输出h(t)。紧接着下一个timestep，x(t+1)、h(t)和c(t)继续输入到cell，输出为h(t+1)和c(t+1)，如下图。因此，Keras中的return_sequences和return_state，就是跟h(t)和c(t)相关。 Return Sequences接下来我们来点hands-on的代码，来具体看看这两个参数的作用。 实验一试验代码中，return_sequences和return_state默认都是false，输入shape为(1,3,1)，表示1个batch，3个timestep，1个feature 123456789101112from keras.models import Modelfrom keras.layers import Inputfrom keras.layers import LSTMfrom numpy import array# define modelinputs1 = Input(shape=(3, 1))lstm1 = LSTM(1)(inputs1)model = Model(inputs=inputs1, outputs=lstm1)# define input datadata = array([0.1, 0.2, 0.3]).reshape((1,3,1))# make and show predictionprint(model.predict(data)) 输出结果为 1[[-0.0953151]] 表示在经历了3个time step的输入后，LSTM返回的hidden state，也就是上文中的h(t)。由于输出的是网络最后一个timestep的值，因此结果是一个标量。 实验二我们加上参数return_sequences=True 1lstm1 = LSTM(1, return_sequences=True)(inputs1) 输出结果为 123[[[-0.02243521][-0.06210149][-0.11457888]]] 我们看到，输出了一个array，长度等于timestep，表示网络输出了每个timestep的h(t)。 总结一下，return_sequences即表示，LSTM的输出h(t)，是输出最后一个timestep的h(t)，还是把所有timestep的h(t)都输出出来。在实际应用中，关系到网络的应用场景是many-to-one还是many-to-many，非常重要。 Return State实验三接下来我们继续实验return_state 1lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1) 输出结果为 123[array([[ 0.10951342]], dtype=float32), array([[ 0.10951342]], dtype=float32), array([[ 0.24143776]], dtype=float32)] 注意，输出是一个列表list，分别表示 最后一个time step的hidden state 最后一个time step的hidden state（跟上面一样) 最后一个time step的cell state（注意就是上文中的c(t)） 可以看出，return_state就是控制LSTM中的c(t)输出与否。 实验四我们最后看看return_sequences和return_state全开的情况。 1lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True) 输出结果为 12345[array([[[-0.02145359], [-0.0540871 ], [-0.09228823]]], dtype=float32), array([[-0.09228823]], dtype=float32), array([[-0.19803026]], dtype=float32)] 输出列表的意义其实跟上面实验三一致，只是第一个hidden state h(t)变成了所有timestep的，因此也是长度等于timestep的array。 Time Distributed最后再讲一讲Keras中的TimeDistributed。这个也是在RNN中非常常用但比较难理解的概念，原作者解释说 TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor. 其实它的主要用途在于Many-to-Many： 比如输入shape为(1, 5, 1)，输出shape为(1, 5, 1) 123model = Sequential()model.add(LSTM(3, input_shape=(length, 1), return_sequences=True))model.add(TimeDistributed(Dense(1))) 根据上面解读，return_sequences=True，使得LSTM的输出为每个timestep的hidden state，shape为(1, 5, 3) 现在需要将这个(1 ,5, 3)的3D tensor变换为(1, 5, 1)的结果，需要3个Dense layer，分别作用于每个time step的输出。而使用了TimeDistributed后，则把一个相同的Dense layer去分别作用，可以使得网络更为紧凑，参数更少的作用。 如果是在many-to-one的情况，return_sequence=False，则LSTM的输出为最后一个time step的hidden state，shape为(1, 3)。此时加上一个Dense layer, 不用使用TimeDistributed，就可以将(1, 3)变换为(1, 1)。 总结本文主要通过一些实际的代码案例，解释了Keras的LSTM API中常见的两个参数return_sequence和return_state的原理及作用，在Tensorflow及PyTorch，也有相通的，希望能够帮助大家加深对RNN的理解。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不用for循环！教你用numpy“手撕”朴素贝叶斯]]></title>
    <url>%2Fposts%2Fnumpy-naive-bayes%2F</url>
    <content type="text"><![CDATA[常见的python朴素贝叶斯算法的方式，都是使用for循环来统计各个p(特征|类型)的值。其实机器学习除了常规算法思路外，很关键且很优雅的地方在于矩阵化（向量化），即vectorization。通过各种矩阵运算来去除for循环，是目前机器学习、深度学习中非常关键的技巧。文本不使用各种高阶机器学习库，单纯使用numpy来手撕朴素贝叶斯算法，带你领略机器学习中的矩阵运算之道。 数据准备为了demo方便，我们先伪造一些数据。这里伪造的是NLP情感分类的数据。分为两部门train和valid，标签分为1和0，1代表侮辱性文字，0代表正常言论。 12345678train = ['my dog had flea problems help help please', 'mybe not take him to dog park stupid dog', 'my dalmation is so cute I love him', 'stop posting stupid worthless garbage', 'mr licks ate my steak how to stop him',]valid = ['my stupid stupid worthless dog']label = [0,1,0,1,0]valid_y = [1] 数据预处理NLP的数据预处理主要就是词典的创建和文本的向量化。这里由于是英语文档，不存在分词的问题。 这里由于是手撕教程，不使用spacy等高阶NLP库，词典创建的也即使用Python的set 123456vocab = set([])for doc in train: vocab = vocab | set(doc.split())vocabList = list(vocab)# 根据词反查indexvocabList.index('please') 文档向量化向量话的原理，首先将文档中句子的每一个单词按照字典转换为index 123456train_vec = []valid_vec = []for doc in train: train_vec.append([vocabList.index(x) for x in doc.split()])for doc in valid: valid_vec.append([vocabList.index(x) for x in doc.split()]) 可以看看向量化后的结果 123456789train_vec, valid_vec'''[[28, 11, 16, 7, 26, 19, 19, 8], [20, 24, 21, 14, 6, 11, 12, 22, 11], [28, 5, 9, 25, 0, 3, 2, 14], [1, 17, 22, 15, 13], [18, 4, 27, 28, 10, 23, 6, 1, 14]], [[28, 22, 22, 15, 11]]''' 接着是将词典的每一个单词作为一个feature维度处理，将所有文档处理成相同维度的矩阵，维度大小即为词典的长度，而每个维度的值有多种处理方式，比如按照句子中每个单词的Count计数，或者是每个单词的TFIDF值。这里我们采用了Count，为了体现手撕，使用了Python的Counter，为了体现numpy矩阵运算的思路，使用了scipy的稀疏矩阵sparse.csr_matrix。 1234567891011121314151617from collections import Counterfrom scipy.sparse import csr_matrixdef get_term_doc_matrix(label_list, vocab_len): j_indices = [] indptr = [] values = [] indptr.append(0) for i, doc in enumerate(label_list): feature_counter = Counter(doc) j_indices.extend(feature_counter.keys()) values.extend(feature_counter.values()) indptr.append(len(j_indices)) return csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, vocab_len), dtype=int)train_matrix = get_term_doc_matrix(train_vec, len(vocabList)valid_matrix = get_term_doc_matrix(valid_vec, len(vocabList) 可以看看矩阵化后的train和valid，分别转换成了samples*feature，29即为字典的维度。 1234567train_matrix, valid_matrix'''&lt;5x29 sparse matrix of type '' with 37 stored elements in Compressed Sparse Row format&gt;, &lt;1x29 sparse matrix of type '' with 4 stored elements in Compressed Sparse Row format&gt;''' 我们将生成的train_matrix作图如下（为方便观看省去了部分维度） 朴素贝叶斯算法原理谈到朴素贝叶斯算法，首先想到的必定是著名的贝叶斯公式 不过这个定理如何体现到我们机器学习的场景呢？我们把其中的A和B换成实际的物理意义如下： 在我们的分类监督学习中，其实就是求在一定特征的样本情况下，某个具体类别的概率。根据贝叶斯公式，可以转换成分别求某个类别下这些特征的概率、某类别的概率、特征的概率。 实际场景中，特征一般是多维的，而多维特征的联合概率是比较复杂的。这时候，就体现出朴素的概念了。我们对样本作出假设：样本中的各个特征是相互独立的。这样，可以将联合概率转换成以下的独立概率乘积： 当然，这种假设肯定是粗暴的。但是实践过程中，计算量减少带来的益处是远大于粗暴假设带来的失真。这也是最考验算法工程师的地方，既要考虑数学的严谨，同时也要考虑工程实现。“part-science, part-art”，这或许也是机器学习之道。 当然，对于模型最终的应用来说，求出具体的概率绝对值是意义不大的。我们只需要知道不同特征之间概率的比值就行。也就是说，对于二分类问题，我们只需要知道： 就可以判断出该样本是属于类别1。于是，我们可以接着进行公式变换，将P(类别2|特征)移到等式左边，并取log： 同样，根据朴素的假设，可以得到： 即，我们需要求出每个特征对应的 以及每个类别对应的 然后全部相加，看结果是否大于0即可判断出样本的类别。而这两块，我们都是可以从训练样本中求出（先验概率），这也就是朴素贝叶斯算法训练的部分。 ##朴素贝叶斯矩阵运算 前言中说到，一般的教程，直接使用for循环数数一般就可以求出上述的两个log值，完成“训练”。我们这样，要充分应用numpy的矩阵运算特性，不使用for循环 123456import numpy as npp1 = np.squeeze(np.asarray(train_matrix[np.asarray(label==1].sum(0)))p0 = np.squeeze(np.asarray(train_matrix[np.asarray(label==0].sum(0)))pr1 = (p1+1) / ((np.asarray(label)==1).sum() + 1)pr0 = (p0+1) / ((np.asarray(label)==0).sum() + 1)r = np.log(pr1/pr0) 还是以这个图为例，pr1即为类别为1的样本中特征x的概率。在NLP中，每个特征都是一个词，其物理意义就是某个词在某个类别中出现的概率，而最后求出的r即为所有特征的概率在2个类别的比值，注意，这里的r是个向量，是通过样本之间的矩阵运算一次性求出。 123456789r'''array([-0.40546511, 0.28768207, -0.40546511, -0.40546511,-0.40546511, -0.40546511, 0.28768207, -0.40546511, -0.40546511, -0.40546511, -0.40546511, 0.69314718, 0.98082925, 0.98082925, -0.11778304, 0.98082925, -0.40546511, 0.98082925, -0.40546511, -0.81093022, 0.98082925, 0.98082925, 1.38629436, -0.40546511, 0.98082925, -0.40546511, -0.40546511, -0.40546511, -1.09861229])''' 接下来求第二个log，这个直接就是不同类别的比例。 1234b = np.log((np.asarray(label)==1).mean() / (np.asarra(label)==0).mean())'''-0.4054651081081643''' 这里的r和b即为我们训练出的模型的参数。可以将其序列化到磁盘，供后续预测的时候使用（也就是深度学习中的inference）。 朴素贝叶斯推理由于r代表了训练样本中每一个特征词的在不同类别的概率比值，比如’stupid’等词就很高，而’love’等词就较低。在实际应用中，我们同样取出预测样本中的所有词，然后考虑这些词的概率比值，最后全部相加，即可判断最终样本的类别。同样，我们使用numpy的矩阵运算。 1234train_matrix@r + b &gt; 0# array([False, True, False, True, False])valid_matrix@r + b &gt; 0# array([ True]) 这里@是numpy的矩阵乘操作，举例如图而b作为一个标量，会直接触发numpy中的broadcast操作，直接把+应用到所有样本，同样&gt;也会broadcast，最终得到一个array，表示每一个测试样本是否预测为类别1。 扩展由于上面推理过程中，train_matrix和valid_matrix都是由词频组成，相当于在求具体概率比值的时候应用词频做了加权。在实际应用中，也可考虑不使用词频，直接使用是否含有某个特征词，如下： 12train_matrix.sign()@r + b &gt; 0valid_matrix.sign()@r + b &gt; 0 具体哪种方式好，还是要根据实际的数据样本情况来具体测试。 总结本文以NLP中的句子情感分析为例，使用numpy的矩阵运算来手撕朴素贝叶斯算法，可以加深对与朴素贝叶斯算法的理解，并体会机器学习中矩阵运算之道。 朴素贝叶斯简单快捷，特别适合较大规模的稀疏矩阵，在情感分析、垃圾邮件分类等场景中有很普遍的应用。在深度学习大行其道的今天，我们仍然可以通过这个简单的算法来快速搭建整个流程的pipeline。在我眼里，在机器学习项目的初期，流程pipeline的重要性大于任何算法。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我要开始写作？]]></title>
    <url>%2Fposts%2Fwhy-start-writting%2F</url>
    <content type="text"><![CDATA[源起– 说起博客，在大学阶段曾经玩过，当时玩的还是MSN Spaces。不过上面更多是一些经历、感悟，为赋新词强说愁罢了。后来读了研究生、有了家庭、有了事业，个人博客也就逐渐荒废了。现在MSN Spaces早已关闭，之前的文章也就无从寻觅了。工作以后，写作方面更多的是工作记录，以零散笔记为主，主要是用的Evernote以及现在的Bear，但是太过零散不成体系。 无意之中看到了Andrew Chen的一篇推文，“关于写作的15条推特”，深得我心，也是我为什么想在2019年开始写作的最好诠释。 Andrew Chen是在用户增长(User Growth)方面的专家，也曾任Uber的增长VP。以下是他的推文原文，并附上我的思考。 Andrew Chen’s TweetstormTweet #1 1/ After 10+ years of publishing professional writing at http://andrewchen.co , I have a couple opinions on how to get your stuff read. 在过去十年的工作生涯中，我有着非常多的经历，从软件工程师到Scrum Master，从数据分析到大数据挖掘，从机器学习到深度学习。而这些经历更多的是散落在各处，没有一个地方可以沉淀下来。当我意识到这是个很严重的问题时，我也开启了我自己的个人网站machinelearningzen.com的建设，希望能够汇聚我的经历和感悟，并把它们分享出来。 Tweet #2 2/ Titles are 80% of the work, but you write it as the very last thing. It has to be an compelling opinion or important learning 现在人的生活节奏都非常的快，这也是“标题党”横行的客观原因。但是我觉得除了有一个引人入胜的标题外，还是要有能够撑得起这个标题的干货内容才行。 Tweet #3 3/ There’s always room for high-quality thoughts/opinions. Venn diagram of people w/ knowledge and those we can communicate is tiny 这个也应证了“内容为王”。现在虽是自媒体时代，但是各种抄袭、伪原创仍然遍地都是。我希望我自己的内容都是自己经过深度思考后的输出，而不是东拼西凑。 Tweet #4 4/ Writing is the most scalable professional networking activity - stay home, don’t go to events/conferences, and just put ideas down 非常赞同，感谢互联网和移动互联网，我们可以呆在家里或者星巴克，一杯咖啡，就能跟全世界志同道合的朋友交流。 Tweet #5 5/ Think of your writing on the same timescale as your career. Write on a multi-decade timeframe. This means, don’t just pub on Quora/Medium 写作是一个非常好的把自己的碎片知识体系化、融会贯通的方式。以十年为单位日积月累，你的知识体系会像参天大树一样。 Tweet #6 6/ Focus on writing freq over anything else. Schedule it. Don’t worry about building an immediate audience. Focus on the intrinsic. 专注于内容本身，不用担心没有读者。另外，保持写作的频率，人都会有“拖延症”，有些事情，一旦开始拖就等于结束了。 Tweet #7 7/ To develop the habit, put a calendar reminder each Sunday for 2 hours. Forced myself to stare at a blank text box and put something down 养成习惯，必须要有很强的计划力和执行力。这些能力在工作当中同样重要，需要一辈子的修行。 Tweet #8 8/ Most of my writing comes from talking/reading deciding I strongly agree or disagree. These opinions become titles. Titles become essays. 保持阅读和交流非常重要。我的工作生涯的前几年，由于圈子太窄，接触的信息也很少，导致成长非常缓慢。后来注意到这个问题后，开始大量的阅读和交流，自己也开始了突飞猛进。现在，是时候把这些阅读和交流再继续沉淀成文字，给更多的人分享。 Tweet #9 9/ People are often obsessed with needing to write original ideas. Forget it. You’re a journalist with a day job in the tech industry 不用拘泥于开天辟地，现在的科技领域已经不是个人单打独斗的时代了。学会踩在巨人的肩膀上，你才能看得更远。 Tweet #10 10/ An email subscriber is worth 100x twitter or LinkedIn followers or whatever other stuff is out there. An email = a real channel Email 订阅在国外非常流行，可是在中国一直发展不起来，或许就是人们的使用习惯导致。好在移动互联网时代，微信公众号也是一个真正的推送渠道。我也开通了自己的微信公众号，文章和个人网站同步，希望喜欢听我的分享的朋友可以通过这种方式得到最新文章的推送。 Tweet #11 11/ I started writing while working at a VC. They asked, “Why give away ideas? That’s your edge.” Ironic that VCs blog/tweet all day now ;) 这个也是很多人不愿意写博客分享的原因，害怕自己的idea被别人抄袭。其实写文章分享idea，获得巨大提升的还是自己。 Tweet #12 12/ Publishing ideas, learnings, opinions, for years &amp; years is a great way to give. And you’ll figure out how to capture value later 年复一年地发表你的想法、学到的知识、观点，你很快就能在分享之后获得其中的价值。虽然我才开始真正的写作，我已经感受到了自己的成长，我会一直把它持续下去。 Tweet #13 13/ Today I learned that tweetstorms can be way less structured than writing an essay, which makes it much easier ;) This is my first one! Andrew Chen认为在twitter上写作是另外一种轻松愉快的方式，他把这种方式命名为“推特风暴” Tweet #14 14/ OK that’s all for now! Thanks for reading :) :) Tweet #15 15/ Bonus: In order of value, Writing &gt; Reading books &gt; Reading reddit &gt; Twitter &gt; FB/Instagram ;) 这个是Andrew认为的价值排序：写作 &gt; 读书 &gt; 读 reddit &gt; Twitter &gt; Facebook/Instagram。当然这是站在个人提升的价值上的，切记，我们的工作不等于我们的生活。保持自己1～2个爱好，并持续投入成为达人，你的人生会更加美好。]]></content>
      <categories>
        <category>感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy“手撕”文本主题模型之LSA]]></title>
    <url>%2Fposts%2Fnumpy-lsa%2F</url>
    <content type="text"><![CDATA[前言在前文中，我们采用了一个tf-idf来获取本文的关键词（主题）。由于tf-idf算法仅仅是一个统计模型，简单快速，适合作为baseline。它最大的问题在于单单考虑了词频，而没有考虑语义，并且我们取topK的词，这个K值如何选取，也是一个问题。 本文介绍一种真正的主题模型，也是最早出现的主题模型LSA（Latent Semantic Analysis）：潜在语义分析，它主要是利用SVD降维的方式，将词与本文映射到一个新的空间，而这个空间正是以主题作为维度。它的原理非常漂亮，一次奇异值分解就可以得到主题模型，同时也解决了词义的问题。 本文继续发挥hands-on的传统，以一个实例来说明LSA的用法。在生产环境中，一般会使用gensim等框架来快速进行开发，本文从scipy和numpy入手，可以更清楚的了解其中的原理。 数据预处理为了演示方便，我们直接采用了scikit-learn中的Newsgroups数据集。这是用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。数据集收集了大约20,000左右的新闻组文档，均匀分为20个不同主题的新闻组集合。 我们截取了其中4个主题的数据，并采用scikit-learn中的API来装载。 12345from sklearn.datasets import fetch_20newsgroupscategories = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']remove = ('headers', 'footers', 'quotes')newsgroups_train = fetch_20newsgroups(subset='train',categories=categories, remove=remove)newsgroups_test = fetch_20newsgroups(subset='test',categories=categories, remove=remove) 数据下载好后，我们看看里面的文本长啥样 12345678910111213141516171819newsgroups_train.data[:5]'''["Hi,\n\n I've noticed that if you only save a model (withall your mapping planes\n positioned carefully) to a .3DSfile that when you reload it after restarting\n 3DS, theyare given a default position and orientation. But if yousave\nto a .PRJ file their positions/orientation arepreserved. Does anyone\n know why this information is notstored in the .3DS file? Nothing is\nexplicitly said inthe manual about saving texture rules in the .PRJ file.\n I'd like to be able to read the texture rule information,does anyone have \n the format for the .PRJ file?\n\n Is the.CEL file format available from somewhere?\n\n Rych", '\n\n Seems to be, barring evidence to the contrary, that Koresh was simply\n another deranged fanatic who thought it neccessary to take a whole bunch of\n folks with him, children and all, to satisfy his delusional mania. Jim\n Jones, circa 1993.\n\n\n Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n for centuries.']''' 拿到文本后，第一件事当然是tokenizer，然后采用bag-of-words词袋模型将其向量化。这里使用scikit-learn中的CountVectorizer，以词频计数来作为向量值，当然更精细化可以采用TfidfVectorizer。 12345import numpy as npfrom sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer(stop_words='english')vectors = vectorizer.fit_transform(newsgroups_train.data.todense()vocab = np.array(vectorizer.get_feature_names()) 可以看看向量化后的矩阵 1234vectors.shape'''(2034, 26576)''' 这里表示共有2034篇文档，词的vocab_size为26576，将所有文档转成了26576维的向量，每个向量的值为该维表示词的词频。 SVD分解向量化完成后，我们就可以开始奇异值分解了。 12import scipyU, s, Vh = scipy.linalg.svd(vectors, full_matrices=False) 根据上图，我们可以看到分解出来的矩阵： U: (2034, 2034) 表示2034个样本，对应2034个topic s: (2034, ) 表示2034个奇异值，即topic的重要性分数 Vh: (2034, 26576) 表示2034个topic，对应26576个vocab，注意这个Vh是公式中的VT，即转置后的V SVD奇异值分解，具有以下性质： 奇异值分解为精确分解，即分解后的矩阵可以完全还原原矩阵，信息不丢失 U和Vh是正交矩阵 我们可以在numpy中验证一下： 1234# 注意将s从一维向量转换成对角矩阵(diagonal matrix)np.allclose(U.dot(np.diag(s)).dot(Vh), vectors)np.allclose(U.T.dot(U), np.eye(U.shape[0]))np.allclose(Vh.dot(Vh.T), np.eye(Vh.shape[0])) Topic解读LSA的优雅之处，就是把之前的高维文档向量，降维到低维，且这个维度代表了文档的隐含语义，即这个文档的主题topic。svd分解出来的Vh矩阵，即是每个主题的矩阵，维度是每个单词，维度值可以看成是这个主题中每个单词的的重要性。那么，我们可以选取重要性最高的词，来解读某个隐含主题。 123456789101112131415161718num_top_words = 8def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a])return [' '.join(t) for t in topic_words]show_topics(Vh[:10])'''['ditto critus propagandist surname galacticentrickindergarten surreal imaginative', 'jpeg gif file color quality image jfif format', 'graphics edu pub mail 128 3d ray ftp', 'jesus god matthew people atheists atheism does graphics', 'image data processing analysis software available tools display', 'god atheists atheism religious believe religion argument true', 'space nasa lunar mars probe moon missions probes', 'image probe surface lunar mars probes moon orbit', 'argument fallacy conclusion example true ad argumentum premises', 'space larson image theory universe physical nasa material']''' 可以看出，有些主题是关于图片格式的，有的是关于邮件协议的，有的是关于太空的。 而svd分解出来的U矩阵，就是每个文档对应的主题矩阵，维度是每个主题，维度值也可以看成是每个主题的重要性。 Truncated SVD在生产实践中，普通svd由于要exact decomposition，计算量会非常大，且最后的应用往往会只看前n个topic，因此Truncated SVD的优势在于，预先设置好n的值，可以在牺牲一定精度的条件下，大大减少计算量。这里提供两种常见的svd实现以作参考。 sklearn实现12from sklearn import decompositionu, s, v = decomposition.randomized_svd(vectors, 10) Facebook实现12import fbpcau, s, v = fbpca.pca(vectors, 10) 总结LSA作为最早出现的真正主题模型，非常优雅，同时也存在很多不足，比如它得到的不是一个概率模型，同时得到的向量中含有负值，难以直观解释。针对这些问题，后续就有NMF（非负矩阵分解），LDA（隐含狄利克雷分布）等模型出现。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LSA</tag>
        <tag>SVD</tag>
        <tag>Topic Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你快速入门知识图谱 - Neo4J教程]]></title>
    <url>%2Fposts%2Fknowledge-graph-neo4j-toturial%2F</url>
    <content type="text"><![CDATA[前言今天，我们来聊一聊知识图谱中的Neo4J。首先，什么是知识图谱？先摘一段百度百科： 知识图谱（Knowledge Graph），在图书情报界称为知识域可视化或知识领域映射地图，是显示知识发展进程与结构关系的一系列各种不同的图形，用 可视化技术描述知识资源及其载体，挖掘、分析、 构建、绘制和显示知识及它们之间的相互联系。知识图谱是通过将应用数学、 图形学、信息可视化技术、 信息科学等学科的理论与方法与计量学引文分析、共现分析等方法结合，并利用可视化的图谱形象地展示学科的核心结构、发展历史、 前沿领域以及整体知识架构达到多学科融合目的的现代理论。它能为学科研究提供切实的、有价值的参考。 简单说来，知识图谱就是通过不同知识的关联性形成一个网状的知识结构，而这个知识结构，恰好就是人工智能AI的基石。当前AI领域热门的计算机图像、语音识别甚至是NLP，其实都是AI的感知能力，真正AI的认知能力，就要靠知识图谱。 知识图谱目前的应用主要在搜索、智能问答、推荐系统等方面。知识图谱的建设，一般包括数据获取、实体识别和关系抽取、数据存储、图谱应用都几个方面。本文着眼于数据存储这块，给大家一个Neo4J的快速教程。 Neo4J简介知识图谱由于其数据包含实体、属性、关系等，常见的关系型数据库诸如MySQL之类不能很好的体现数据的这些特点，因此知识图谱数据的存储一般是采用图数据库（Graph Databases）。而Neo4j是其中最为常见的图数据库。 Neo4J安装首先在 https://neo4j.com/download/ 下载Neo4J。Neo4J分为社区版和企业版，企业版在横向扩展、权限控制、运行性能、HA等方面都比社区版好，适合正式的生产环境，普通的学习和开发采用免费社区版就好。 在Mac或者Linux中，安装好jdk后，直接解压下载好的Neo4J包，运行bin/neo4j start即可 Neo4J使用Neo4J提供了一个用户友好的web界面，可以进行各项配置、写入、查询等操作，并且提供了可视化功能。类似ElasticSearch一样，我个人非常喜欢这种开箱即用的设计。 打开浏览器，输入http://127.0.0.1:7474/browser/，如下图所示，界面最上方就是交互的输入框。 Cypher查询语言Cypher是Neo4J的声明式图形查询语言，允许用户不必编写图形结构的遍历代码，就可以对图形数据进行高效的查询。Cypher的设计目的类似SQL，适合于开发者以及在数据库上做点对点模式（ad-hoc）查询的专业操作人员。其具备的能力包括： 创建、更新、删除节点和关系 通过模式匹配来查询和修改节点和关系 管理索引和约束等 Neo4J实战教程直接讲解Cypher的语法会非常枯燥，本文通过一个实际的案例来一步一步教你使用Cypher来操作Neo4J。 这个案例的节点主要包括人物和城市两类，人物和人物之间有朋友、夫妻等关系，人物和城市之间有出生地的关系。 1. 首先，我们删除数据库中以往的图，确保一个空白的环境进行操作： 1MATCH (n) DETACH DELETE n 这里，MATCH是匹配操作，而小括号()代表一个节点node（可理解为括号类似一个圆形），括号里面的n为标识符。 2. 接着，我们创建一个人物节点： 1CREATE (n:Person &#123;name:&apos;John&apos;&#125;) RETURN n CREATE是创建操作，Person是标签，代表节点的类型。花括号{}代表节点的属性，属性类似Python的字典。这条语句的含义就是创建一个标签为Person的节点，该节点具有一个name属性，属性值是John。 如图所示，在Neo4J的界面上可以看到创建成功的节点。3. 我们继续来创建更多的人物节点，并分别命名： 12345CREATE (n:Person &#123;name:&apos;Sally&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Steve&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Mike&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Liz&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Shawn&apos;&#125;) RETURN n 如图所示，6个人物节点创建成功4. 接下来创建地区节点 12345CREATE (n:Location &#123;city:&apos;Miami&apos;, state:&apos;FL&apos;&#125;)CREATE (n:Location &#123;city:&apos;Boston&apos;, state:&apos;MA&apos;&#125;)CREATE (n:Location &#123;city:&apos;Lynn&apos;, state:&apos;MA&apos;&#125;)CREATE (n:Location &#123;city:&apos;Portland&apos;, state:&apos;ME&apos;&#125;)CREATE (n:Location &#123;city:&apos;San Francisco&apos;, state:&apos;CA&apos;&#125;) 可以看到，节点类型为Location，属性包括city和state。 如图所示，共有6个人物节点、5个地区节点，Neo4J贴心地使用不用的颜色来表示不同类型的节点。 5. 接下来创建关系 123MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Person &#123;name:&apos;Mike&apos;&#125;) MERGE (a)-[:FRIENDS]-&gt;(b) 这里的方括号[]即为关系，FRIENDS为关系的类型。注意这里的箭头--&gt;是有方向的，表示是从a到b的关系。 如图，Liz和Mike之间建立了FRIENDS关系，通过Neo4J的可视化很明显的可以看出： 6. 关系也可以增加属性 123MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Person &#123;name:&apos;Sally&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2001&#125;]-&gt;(b) 在关系中，同样的使用花括号{}来增加关系的属性，也是类似Python的字典，这里给FRIENDS关系增加了since属性，属性值为2001，表示他们建立朋友关系的时间。 7. 接下来增加更多的关系 1234MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Person &#123;name:&apos;John&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2012&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;), (b:Person &#123;name:&apos;Shawn&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2006&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Sally&apos;&#125;), (b:Person &#123;name:&apos;Steve&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2006&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Person &#123;name:&apos;John&apos;&#125;) MERGE (a)-[:MARRIED &#123;since:1998&#125;]-&gt;(b) 如图，人物关系图已建立好，有点图谱的意思了吧？ 8. 然后，我们需要建立不同类型节点之间的关系-人物和地点的关系 1MATCH (a:Person &#123;name:&apos;John&apos;&#125;), (b:Location &#123;city:&apos;Boston&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1978&#125;]-&gt;(b) 这里的关系是BORN_IN，表示出生地，同样有一个属性，表示出生年份。 如图，在人物节点和地区节点之间，人物出生地关系已建立好。 9. 同样建立更多人的出生地 1234MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Location &#123;city:&apos;Boston&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1981&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;), (b:Location &#123;city:&apos;San Francisco&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1960&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Location &#123;city:&apos;Miami&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1960&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Steve&apos;&#125;), (b:Location &#123;city:&apos;Lynn&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1970&#125;]-&gt;(b) 建好以后，整个图如下 10. 至此，知识图谱的数据已经插入完毕，可以开始做查询了。我们查询下所有在Boston出生的人物 1MATCH (a:Person)-[:BORN_IN]-&gt;(b:Location &#123;city:&apos;Boston&apos;&#125;) RETURN a,b 结果如图 11. 查询所有对外有关系的节点 1MATCH (a)--&gt;() RETURN a 注意这里箭头的方向，返回结果不含任何地区节点，因为地区并没有指向其他节点（只是被指向） 12. 查询所有有关系的节点 1MATCH (a)--() RETURN a 结果如图 13. 查询所有对外有关系的节点，以及关系类型 1MATCH (a)-[r]-&gt;() RETURN a.name, type(r) 结果如图 14. 查询所有有结婚关系的节点 1MATCH (n)-[:MARRIED]-() RETURN n 结果如图 15. 创建节点的时候就建好关系 1CREATE (a:Person &#123;name:&apos;Todd&apos;&#125;)-[r:FRIENDS]-&gt;(b:Person &#123;name:&apos;Carlos&apos;&#125;) 结果如图 16. 查找某人的朋友的朋友 1MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;)-[r1:FRIENDS]-()-[r2:FRIENDS]-(friend_of_a_friend) RETURN friend_of_a_friend.name AS fofName 返回Mike的朋友的朋友：从图上也可以看出，Mike的朋友是Shawn，Shawn的朋友是John和Sally 17. 增加/修改节点的属性 1234MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;) SET a.age=34MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;) SET a.age=32MATCH (a:Person &#123;name:&apos;John&apos;&#125;) SET a.age=44MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) SET a.age=25 这里，SET表示修改操作 18. 删除节点的属性 12MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) SET a.test=&apos;test&apos;MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) REMOVE a.test 删除属性操作主要通过REMOVE 19. 删除节点 1MATCH (a:Location &#123;city:&apos;Portland&apos;&#125;) DELETE a 删除节点操作是DELETE 20. 删除有关系的节点 1MATCH (a:Person &#123;name:&apos;Todd&apos;&#125;)-[rel]-(b:Person) DELETE a,b,rel 总结本文重点针对常见的知识图谱图数据库Neo4J进行了介绍，并且采用一个实际的案例来说明Neo4J的查询语言Cypher的使用方法。 当然，类似MySQL一样，在实际的生产应用中，除了简单的查询操作会在Neo4J的web页面进行外，一般还是使用Python、Java等的driver来在程序中实现。后续会继续介绍编程语言如何操作Neo4J。]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Neo4J</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代NLP的基石 - Word2Vec]]></title>
    <url>%2Fposts%2Fnlp-word2vec%2F</url>
    <content type="text"><![CDATA[起源机器学习的各项算法，都可以简单理解为：一个向量vector，经过一个函数f(x)，结果得到另一个向量vector或者标量scalar。输入的这个向量，我们称之为样本sample，向量中的每一个值，代表了样本的每一维特征feature。这个向量一般是浮点数的，即dtype=np.float32。 在NLP的相关领域中，我们的样本往往是一段文本内容，如一个句子、一篇文章等等，我们不能把这些文本直接喂给机器学习模型，而是要先把文本转换成浮点数的向量，即文本的向量化表示。这在NLP领域中是非常基础且重要的阶段，其结果可以用在文档搜索，网页搜索，垃圾邮件过滤，文本主题建模等等方面。 BOW在文本的向量话表示领域中，最早出现的方法是BOW（Bag Of Words），即词袋模型。这个模型利用onehot编码的思想，把每一个词看作是一个维度，然后一个文档或句子就可以表示成一个维度为N的向量，N为语料中词语的个数。每个维度具体的值，可以通过简单计数或者计算TF-IDF值来得出。实际方案可参考Scikit Learn的CountVectorizer和TfidfVectorizer。 这种方式简单，但是效果一般，因为它忽略了很多信息，比如在一个句子中词语的顺序。另外，由于每个词语都是独立的维度，词与词之间的距离（相关）都是相等的，这也明显不合理。 Word2VecWord2vec 1301.3781 Efficient Estimation of Word Representations in Vector Space是这个领域非常重要的理念。我认为，Word2Vec是NLP迈向深度学习的基石，通过embedding的方法，让模型能够理解到每个词背后的隐藏含义。 Word2Vec认为，每一个词都可以表示为一个密集多维向量（相比BOW中的稀疏向量），且这个向量可以捕捉词与词之间的关联。显然，密集的浮点向量，对于机器学习模型是非常友好的。 Word2Vec除了采用密集浮点向量来表示每一个单词外，还能实现一个特殊的功能，把数学运算和文本结合起来，比如著名的“king-man-queen-woman”图如下所示： 采用Word2Vec表示以后，我们可以通过矩阵运算，得出 king-man = queen-woman的等式，即词语的向量中，包含了king和queen中关于王权的概念，同时也包含了man和woman的男女之别。 Word2Vec按照实现算法，可分为两类： CBOWCBOW(Continuous Bag Of Words)是实现word2vec的算法之一。它在每个词语上构造了一个滑动窗口，即词语的上下文-词周围的那些词。每个词都用一个固定长度的向量来表示，然后就可以根据词周围的词来预测中间那个词。 在训练过程中，根据每次的预测情况，使用GradientDesent方法，不断调整周围词的向量，当训练完成以后，每个词都会作为中心词，把周围的词向量进行了调整，而这种调整是统一的，即求出的gradient值会同样作用到每个周围词的词向量中。 值得注意的是，在实际操作过程中，周围词的向量会首先通过sum或者concat的方式组合在一起，然后进行预测，在训练过程中，每个epoch计算的次数跟整个文本的词数几乎相等，复杂度大概为O(V)。 更通俗一点的理解，相当于1个老师对应了K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家一样的知识。因此，这种方式对于具体某个学生，学习的结果不一定是最好的，但是整个班级的训练过程是效率最高最快的。 Skip-gramSkip-gram是word2vec的另一种实现算法。它跟cbow完全相反，即使用一个词来预测它周围的词。在skip-gram中，会利用对周围词的预测情况，使用GradientDecent来不断的调整中心词的词向量，最终所有文本遍历完以后，也就得到了文本所有词的词向量。 可以看出，skip-gram的训练次数是要多于cbow的，因为每个词在作为中心词时，都需要把它周围的词全部预测一次，这样相当于比cbow的方法多进行了K-1次（假设K为窗口大小），因此复杂度为O(KV)。 如图所示，这里的窗口值为2，蓝色的词是输入，窗口内的其他词为输出，这样将一个句子转化为训练样本。 在skip-gram当中，每个词都要受到周围词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整，因此，当数据量较少的时候，或者词为生僻词出现次数较少时，这种多次的调整会使得词向量更加的准确。 对比上面的老师与学生的比喻，在skip-gram中，每个词作为中心词时，其实是1个学生 对应了K个老师，K个老师（周围词）都会对学生（中心词）进行训练，这样单个学生的能力就会相对扎实一些，但是这种训练方式整个班级的学习时间肯定更长。 Negative Sample按照正常的训练逻辑，cbow和skip-gram都是分类模型，分类类别数等于语料中词语的个数。因此模型在最后的softmax层非常巨大，会导致训练的时间非常长，且需要非常大的训练数据来收敛模型和避免over-fitting。 举个例子，若有一个包含10000个单词的词汇表，而向量特征为300维，则在最后一层Dense层会有300x10000个权重需要更新。 为了解决这个问题，word2vec的作者提出了2个方案： Subsampling frequent words：以一定的阈值过滤掉了一些高频词，比如“the”，这样可以在一定程度上减少训练数据的数量。 Negative sampling：改变优化目标函数，使得每一个训练样本只更新网络的小部分权重。 举个例子，在训练样本(”fox”, “quick”)时，我们的输出（标签）为一个one-hot向量，且代表”quick”的神经元输出为1，代表其他成千上万的词的神经元输出为0。采用了negative sampling后，我们首先随机选取一小部分”negative”词语（比如5个）去更新网络的权重，而”positive”词语依然也会更新网络权重。如果在网络的输出层，以前是300x10000的矩阵，现在只需要更新1+5=6个单词，即300x6个权重，是以前的0.06%。在Tensorflow中，有现成的辅助函数tf.nn.nce_loss()可供使用。而在具体的negative samples的选取上，使用了“unigram distribution”，即更频繁使用的词语更容易被选取。 总结本文介绍了Word2Vec的起源及相应实现的2种算法cbow和skip-gram，详细对比了这两种算法的原理、训练方式及优缺点，还介绍了Word2Vec训练时的重要trick: negative sampling。Word2Vec虽然不算深度学习，但是它带来的embedding的概念却是现代NLP乃至深度学习的基石，后续我们会继续介绍由此发展的Doc2Vec、Item2Vec、Anything2Vec。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任意网页正文内容主题词提取]]></title>
    <url>%2Fposts%2Fweb-topic-extractor%2F</url>
    <content type="text"><![CDATA[前言网页正文内容主题提取，即任意给一个网页url，通过爬取网页内容和文本分析，得出该网页内容的关键词，作为网页的标签。这些关键词和标签在做流量分析_内容推荐方面有非常重要的意义。比如说我们做数字营销，用过页面来做用户引流，我们就可以知道吸引用户过来的点是什么，用户的潜在需求是什么；另外，针对内容社区的用户画像/推荐系统，关键点也是文章/页面的主题和标签。 这个任务涉及的技术点主要有以下几个： 网页爬虫。做网页内容分析，首先得根据url把网页内容扒下来吧。 正文提取。现在的web页面是非常复杂的，除了正文外，包含了大量的广告、导航、信息流等，我们需要去除干扰，只提取网页的正文信息。 主题模型 。拿到正文文本后，就需要做NLP来提取主题关键字了。 网页爬虫这里的网页爬虫和一般的爬虫还不太一样，会简单许多，主要是把原始网页的HTML抓下来即可，主要是为后续的分析挖掘打下基础，属于数据采集的阶段。 这里我们采用了Python的requests包。requests相对于Python自带的urllib来说，API更为人性化，鲁棒性也更好。 1234import requestsr = request.get(url)r.encoding='utf-8'html = r.text 正文提取通过研究爬取下来的原始HTML，我们可以看到是非常负责而且杂乱无章的，充斥着大量的js代码等。我们首先需要解析HTML，尽量过滤掉js代码，留下文本内容。 这里我们采用了Python的BeautifulSoup包。这个包堪称Python一大神器，解析HTML效果非常好 12345from bs4 import BeautifulSoupsoup = BeautifulSoup(html, features="html.parser")for script in soup(["script", "style"]): script.decompose()text = soup.get_text() 我们想要的是网页的正文内容，其他的诸如广告或者导航栏等干扰内容需要尽可能的过滤掉。通过BeautifulSoup可以解析出整个HTML的DOM树结构，但是每个网页HTML的写法各不相同，单纯靠HTML解析无法做到通用，因此我们需要跳出HTML的思维，使用其他的方法来提取网页的正文。这里有个很优雅的方式是“基于行块分布函数”的算法cx-extractor。 基于行块分布函数的通用网页正文抽取：线性时间、不建DOM树、与HTML标签无关对于Web信息检索来说，网页正文抽取是后续处理的关键。虽然使用正则表达式可以准确的抽取某一固定格式的页面，但面对形形色色的HTML，使用规则处理难免捉襟见肘。能不能高效、准确的将一个页面的正文抽取出来，并做到在大规模网页范围内通用，这是一个直接关系上层应用的难题。作者 提出了 《基于行块分布函数的通用网页正文抽取算法》 ，首次将网页正文抽取问题转化为求页面的行块分布函数，这种方法不用建立Dom树，不被病态HTML所累（事实上与HTML标签完全无关）。通过在线性时间内建立的行块分布函数图，直接准确定位网页正文。同时采用了统计与规则相结合的方法来处理通用性问题。作者相信简单的事情总应该用最简单的办法来解决这一亘古不变的道理。整个算法实现代码不足百行。但量不在多，在法。 上图就是某个页面求出的行块分布函数曲线。该网页的正文区域为145行至182行，即分布函数图上含有最值且连续的一个区域，这个区域往往含有一个骤升点和一个骤降点，因此，网页正文抽取问题转化为了求行块分布函数上的骤升点和骤降点两个边节点。 这里我们采用了这个算法的Python实现GitHub - chrislinan/cx-extractor-python： 1234from CxExtractor import CxExtractorcx = CxExtractor(threshold=40)text = cx.getText(text)texts = text.split('\n') 主题模型拿到网页正文内容文本后，就需要提取正文主题关键词了。常见做法有3种： TFIDF Text-Rank LSI/LDA 这里我们先采用TFIDF的方式来做。 TFIDF(Term Frequency Inverse Document Frequency)是一种用于信息检索与数据挖掘的常用加权技术。 词频（TF）=某个词在文本中出现的次数/该文本中总词数 逆向文档频（IDF）=log(语料库中所有文档总数/(包含某词的文档数+1)) 我们通过TF，也就是某个词在文本中出现的频度，来提升这个词在主题中的权重，然后我们通过IDF值，即逆向文档频来降低公共词的主题权重。TF*IDF也就得到了我们要的主题词权重。 做TFIDF，首先步骤是分词。分词的效果取决于词典的构建，且对后续关键词提取影响巨大。首先要基于分析的行业主题建立专用词典，然后还需要维护停用词的词典。有了词典后，我们就可以采用Python分词的神器jieba来处理分词。 12345678import jiebajieba.load_userdict('./dict.txt') #自定义词典stopwords = set([line.strip() for line in open('stopwords.txt', 'r', encoding='utf-8').readlines()]) #停用词典 word_lists = []for text in texts: word_lists += (list(jieba.cut(text, cut_all=False)))word_lists = [w for w in word_lists if not is_stop_word(w)] 分词完毕后，我们就可以计算TFIDF了。可以通过gensim，scikit-learn等机器学习专用包来做，jieba本身也提供这个功能，这里我们直接用jieba。 12import jieba.analysekeywords = jieba.analyse.extract_tags(' '.join(word_lists), topK=20, withWeight=True, allowPOS=['n', 'ns', 'nr', 'nt', 'nz']) 注意这里有个参数是allowPOS，按照词性过滤。这个需要根据实际的业务需求来设置。 词性标注(Part-Of-Speech Tagging, POS tagging)，是语料库语言学中将语料库内单词的词性按照其含义和上下文内容进行标记的文本数据处理技术。常见标注示例：n 名词nr 人名ns 地名nt 机构团体nz 其他专名a 形容词v 动词 服务到这里，我们的关键词提取就结束了，为了方便其他同学来使用，我们可以用Flask做一个restful api，输入为网址url，输出为提取出的关键词并排序。 总结在这篇文章里，我们完成了从任意网页url提取正文主题关键词的功能。在主题模型这块采用了常见的TFIDF的算法来解决，可以快速出一个原型提供给业务方使用。后续我们会继续优化，采用更多的算法来进一步提升效果。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Topic Model</tag>
        <tag>TFIDF</tag>
      </tags>
  </entry>
</search>
