<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[计算机眼中的数学，跟你所学的不一样]]></title>
    <url>%2Fposts%2Fmath-in-computer%2F</url>
    <content type="text"><![CDATA[Photo by Helloquence on Unsplash 引子先来一道简单的小学数学题：已知函数f(x)定义如下 若x=0.1，将f(x)的结果一直代入f(x)，迭代n次，结果是怎样？ 我相信，任何具有小学数学水平的同学都能解出这道数学题：0.2, 0.4, 0.8, 0.6, 0.2, 0.4, 0.8, 0.6, 0.2, 0.4, 0.8, 0.6, 无限循环 但是我要告诉你，目前人工智能AI的基础-计算机，却无法解出这道题。你信不？ 实验你肯定要说了，不可能！计算机不就是做各种计算的么，各种高等数学微分积分都不在话下，怎么可能这道题都解不出？ OK, talk is cheap, show me the code. 你熟练的拿出MacBook Pro，啪啪啪写下如下代码, 一气呵成： 12345678910def f(x): if x &lt;= 0.5: return 2 * x if x &gt; 0.5: return 2*x - 1x = 0.1for i in range(80): print(x) x = f(x) 伴随着你嘴角上扬的微笑，运行了这段python代码，结果是： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879800.10.20.40.80.60000000000000010.200000000000000180.400000000000000360.80000000000000070.60000000000000140.200000000000002840.40000000000000570.80000000000001140.60000000000002270.200000000000045470.400000000000090950.80000000000018190.60000000000036380.20000000000072760.40000000000145520.80000000000291040.60000000000582080.200000000011641530.400000000023283060.80000000004656610.60000000009313230.200000000186264510.400000000372529030.80000000074505810.60000000149011610.200000002980232240.40000000596046450.8000000119209290.60000002384185790.200000047683715820.400000095367431640.80000019073486330.60000038146972660.200000762939453120.400001525878906250.80000305175781250.6000061035156250.200012207031250.40002441406250.8000488281250.600097656250.20019531250.4003906250.800781250.60156250.2031250.406250.81250.6250.250.51.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.0 傻眼了没？ 为什么你肯定不敢相信自己的眼睛。这在数学上是无法解释，说不通的啊？ 其实，现实就是这么残酷，这是现代计算机的能力和限制。最根本的原因：计算机眼中的数学，跟我们学的数学不一样。 我们学习的数学中的数，是无限的、是连续的；而计算机能够存储和表示的数，是有限的、离散的。 上图就是计算机中存储浮点数的方式，可以看到包括了符号（sign）、数值（mantissa）和指数（exponent）三部分。大家都知道，计算机中的小数常用float来表示，而float32、float64分别代表这这个小数的精度。因此，1.2-1在计算机中并非精确的0.2。不信你再试试: 121.2 - 10.19999999999999996 以有限表示无限、以离散表示连续，难免会造成误差。当我们计算的次数非常多时，误差不断累积，结果就会南辕北辙。 怎么办在普通的计算机程序里面，这种情况还好，因为很多都是整数计算，即使是小数计算，只要计算次数不多，四舍五入以后也看不出问题。 但是，在机器学习和深度学习中，就不是这样了。比如深度学习的基础 - 随机梯度下降，就是各种小数、微分、不断迭代。 因此，为了解决这些问题，有个专门的学科 - 数值方法。 数值方法（numerical method，也称计算方法、数值分析等）是利用计算机进行数值计算来解决数学问题的方法，其研究内容包括数值方法的理论、分析、构造及算法等。很多科学与工程问题都可归结为数学问题，而数值方法对很多基本数学问题建立了数值计算的解决 办法，例如线性代数方程组的求解、多项式插值、微积分和常微分方程的数值解法等等。 通过数值方法，我们可以保证在一定程度上的“数值稳定性 (Numerical Stability)”，即将误差控制在一定范围内。具体操作包括避免除以趋近于零的数，或者避免非常多的小数连续相乘。 在机器学习中，为什么非常多的高手喜欢用log，就有因为log操作能够有这方面的功效。 我们都知道在数学上log有个非常重要的性质: 即可以把乘法变成加法。试想下，如果x和y都是比较小的小数，连续相乘势必会非常影响计算精度，而转化成加法后，就可以把误差控制在一定范围内。 所以说，要搞好AI，不光数学功底要过硬，计算机工程也同等重要。 总结我们平时做算法工作时，在数学上推导出了公式，把公式转化成计算机程序，切记要有数值方法的思维，采用各种操作来减少误差，因为，计算机眼中的数学跟你眼中的不一样。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用numpy来理解PCA和SVD]]></title>
    <url>%2Fposts%2Fnumpy-pca-svd%2F</url>
    <content type="text"><![CDATA[前言线性代数是高等数学里面非常重要的一门课程，可惜在学校的时候是一种自底向上的学习方式，并不知道学出来有什么用，以致彻底沦为应试教育。后来在工作中接触了机器学习，才真正看到了“数学之美”，才发现线性代数是多么的优雅多么的有用。 今天我们来看看下线性代数中非常重要的一个知识点奇异值分解SVD Singular value decomposition。SVD在数据科学当中非常有用，其常见的应用包括： 自然语言处理中的Latent Semantic Analysis 推荐系统中的Collaborative Filtering 降维常用套路Principal Component Analysis LSA已经在前文中有所讲解，CF的话后面在推荐系统的专题中来写，今天主要聊聊PCA，以及SVD在PCA中的重要作用。同样延续我们“手撕”的传统，使用numpy来理解其中的原理。 PCAPrincipal component analysis即主成分分析，是机器学习中一种非常常用的降维方式。其发源也是源自于早期的计算机处理能力有限，当数据样本的维度很高时，预先去除掉数据中的一些冗余信息和噪声（降维），使得数据变得更加简单高效，节省时间和成本。 在深度学习时代，更强调的是原始数据的直接输入，再通过神经网络来做降维工作，最典型是场景就是计算机视觉，直接输入原始图片像素信息，通过CNN卷积层、MaxPooling层来进行降维。因此PCA逐渐开始淡出人们的视线，通常是作为一种数据可视化的手段（二维图表无法展示多维的数据样本）。 其实，在深度学习目前尚未全面攻克的结构化数据领域，PCA仍然有较多的用，其数据处理的思路依然值得我们去学习揣摩。 PCA正常解法PCA算法的本质，其实就是找到一些投影方向，使得数据在这些投影方向上的方差最大，且这些投影方向是相互正交的。找到新的正交基后，计算原始数据在这些正交基上投影的方差，方差越大，就说明对应正交基上包含了更多的信息量。 关于原始数据的方差，最好的一个工具就是协方差矩阵了。协方差矩阵的特征值越大，对应的方差也就越大，在对应的特征向量上投影的信息量就越大。因此，我们如果将小特征值对应方向的数据删除，就可以达到降维的目的。因此，在数学上，我们可以把问题转化为求原始数据的协方差矩阵，然后计算协方差矩阵的特征值与特征向量。 对于广大程序员来说，学习机器学习最重要的一个坎还是数学。很多实际的代码其实是公式推导后的结果的代码实现，如果没有理清公式推导的过程，那么最后肯定是一头雾水。所以，克服心中的恐惧，翻出压在箱底的《线性代数》，我们上。 首先，求原始数据X的协方差矩阵C，将原始矩阵中心化后，做如下操作 接着，由于协方差矩阵C是方阵，就可以通过特征分解的方式来求C的特征值和特征向量。 最后，选择最大的k个特征值进行保留，求X的k阶PCA（X右乘k阶特征向量） 用SVD来解PCA根据上面的推导，我们已经可以对矩阵X做PCA了。同学们可能要问了，这跟SVD有什么关系呢？ 工程化思维强的同学应该已经想到了，这种纯数学的解法，在实际工程实践中有以下问题： 在数据量很大时，把原始矩阵进行转置求协方差矩阵，然后再进行特征值分解是一个非常慢的过程。 稳定性问题。可以看到X转置乘以X，如果矩阵有非常小的数，很容易在平方中丢失。 工业界中，还是“唯快不破，唯稳不破”。我们知道，奇异值分解相对特征分解，有个很大的优势就是不要求原始矩阵是方阵。这非常符合现实生活中的数据。因此，有大神想到，是否可以用svd来解PCA？推导如下： 我们根据协方差矩阵的公式，把X按照奇异值分解展开，注意后面应用到了一个酋矩阵(unitary)的特性： 看到最后的结果，是否跟上面的 很像？没错。协方差矩阵C的特征值和X的奇异值有以下关系 而C的特征向量即为X的SVD分解后的V向量, 则参考PCA正常解法，X的k阶PCA即为_X右乘k阶V向量_。因此这种方式求PCA，只需要把原始矩阵做一次SVD分解即可，不用转置，不用求协方差矩阵。事实上，在Scikit Learn等机器学习框架中，就是用的SVD来做PCA。 用numpy来验证numpy原始解法求PCA接下来，我们用numpy来验证这种思路。首先是PCA的标准解法： 随机模拟一个原始数据矩阵，5个样本，3个特征： 123456789import numpy as npX = np.random.rand(5,3)'''array([[0.86568791, 0.73022945, 0.17982869], [0.07201287, 0.99358411, 0.84389196], [0.61267696, 0.08867997, 0.11770573], [0.16898969, 0.3093472 , 0.9010064 ], [0.43840269, 0.97250927, 0.64897872]])''' 将矩阵中心化，即减去均值： 12345678910X_new = X - np.mean(X, axis=0)'''array([[ 0.43413389, 0.11135945, -0.35845361], [-0.35954115, 0.37471411, 0.30560966], [ 0.18112294, -0.53019003, -0.42057657], [-0.26256433, -0.3095228 , 0.3627241 ], [ 0.00684866, 0.35363927, 0.11069642]])'''# 确保结果正确，即转换后均值为0np.allclose(X_new.mean(axis=0), np.zeros(X_new.shape[1])) 求X_new的协方差矩阵C 1234567C = np.dot(X_new.T, X_new) / (X_new.shape[0] - 1)'''Carray([[ 0.10488363, -0.02467955, -0.10903811], [-0.02467955, 0.16369454, 0.05611495], [-0.10903811, 0.05611495, 0.13564834]])''' 求C的特征值和特征向量，这里用的是numpy的特征分解函数 123456789eig_vals, eig_vecs = np.linalg.eig(C)'''eig_valsarray([0.26474535, 0.00779743, 0.13168373])eig_vecsarray([[-0.53801107, 0.72610049, -0.42816139], [ 0.50584138, -0.12820944, -0.85304562], [ 0.67429117, 0.67552974, 0.29831358]])''' 求X的PCA结果，就是X右乘k阶特征向量。这里k还是取的3。 12345678X_pca = np.dot(X_new, eig_vecs)'''array([[-0.41894072, 0.05880142, -0.38780564], [ 0.58905292, -0.10265648, -0.07453908], [-0.64922927, -0.08462316, 0.24926273], [ 0.22927473, 0.09406657, 0.4846625 ], [ 0.24984234, 0.03441165, -0.27158052]])''' numpy的SVD求PCA首先，直接求X_new的SVD，同样使用numpy的函数 12345678910111213141516# 注意这里的Vh其实是公式中的VTU, Sigma, Vh = np.linalg.svd(X_new, full_matrices=False, compute_uv=True)'''Uarray([[-0.40710685, 0.53434046, 0.33295236, 0.59843699, -0.28241844], [ 0.57241386, 0.10270414, -0.58127362, 0.5471169 , -0.15677466], [-0.63089041, -0.34344824, -0.47916326, 0.32579597, 0.38507162], [ 0.22279838, -0.66779531, 0.53263483, 0.46842625, 0.03587876], [ 0.24278501, 0.37419895, 0.19484969, 0.13026931, 0.86376738]])Sigmaarray([1.02906823, 0.72576506, 0.17660612])Vharray([[-0.53801107, 0.50584138, 0.67429117], [ 0.42816139, 0.85304562, -0.29831358], [ 0.72610049, -0.12820944, 0.67552974]])''' 我们来根据上面的公式，确认下eig_vals和S的关系，注意在numpy的实现中，特征值和奇异值的排序是不同的 1234567np.allclose(eig_vals, np.square(S) / (X_new.shape[0] - 1))'''eig_valsarray([0.26474535, 0.00779743, 0.13168373])np.square(S) / (X_new.shape[0] - 1)array([0.26474535, 0.13168373, 0.00779743])''' 从结果看出，确实跟公式是一致的。 接下来用SVD求PCA就简单了，直接右乘V即可。 1234567891011# 注意Vh是公式中的VT，因此V=Vh.TX_pca_svd = np.dot(X_new, Vh.T)# X_pca_svd = np.dot(U, np.diag(Sigma))'''X_pca_svdarray([[-0.41894072, 0.38780564, 0.05880142], [ 0.58905292, 0.07453908, -0.10265648], [-0.64922927, -0.24926273, -0.08462316], [ 0.22927473, -0.4846625 , 0.09406657], [ 0.24984234, 0.27158052, 0.03441165]])''' 求出结果后，正当我们信心满满的对比一下X_pca和X_pca_svd,以为大功告成打完收工时，却发现二者是不一致的。WTF？ 结果分析仔细研究下X_pca和X_pca_svd的结果，可以看出，排除特征值和奇异值的排序导致的列向量顺序不同外，部分列向量的绝对值相同但正负不同。 问题出在哪里？我们搬出Scikit Learn，再来算一次PCA： 1234567891011from sklearn.decomposition import PCApca = PCA(3)pca.fit_transform(X) # sklearn自动处理去均值化'''array([[ 0.41894072, -0.38780564, -0.05880142], [-0.58905292, -0.07453908, 0.10265648], [ 0.64922927, 0.24926273, 0.08462316], [-0.22927473, 0.4846625 , -0.09406657], [-0.24984234, -0.27158052, -0.03441165]])''' 嗯，也是绝对值相同但正负不同。都说Scikit Learn的PCA就是SVD做的，难道是骗人的？ 好在代码不会骗人，我们直接翻出源码。 通过研究Scikit Learn的源码svd_flip@scikit-learn/extmath.py找到了答案： SVD奇异值分解的结果是唯一的，但是分解出来的U矩阵和V矩阵的正负可以不是唯一，只要保证它们乘起来是一致的就行。因此，sklearn为了保证svd分解结果的一致性，它们的方案是：保证U矩阵的每一行(u_i)中，绝对值最大的元素一定是正数，否则将u_i转成-u_i,并将相应的v_i转成-v_i已保证结果的一致。 这又是数学与工程的问题了。在数学上，几种结果都是正确的。但是在工程上，有个很重要的特性叫幂等性(Idempotence)。 Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. 这是源自于HTTP规范中的一个概念，可以引申至各种分布式服务的设计当中，即：高质量的服务，一次请求和多次请求，其副作用（结果）应当是一致的。Scikit Learn正是通过svd_flip这个函数，把一个数学上并不幂等的操作，转化成了幂等的服务，其设计之讲究可见一斑。 总结本文通过公式推导和numpy代码实战，展示了PCA的正常解法，以及工业界常用的SVD解法，并最后引申至数学和实现的一些探讨。“part-science, part-art”，这就是我最喜爱的机器学习之道]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>SVD</tag>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras中return_sequences和return_state有什么用？]]></title>
    <url>%2Fposts%2Fkeras-return-sequences-return-state%2F</url>
    <content type="text"><![CDATA[Photo by Jon Tyson on Unsplash 前言CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发展的。 今天，我们以LSTM为例，来谈一个RNN中的一个具体的问题。我们知道，在Keras的LSTM实现中，有两个参数return_sequences和return_state。这两个参数的实际意义是什么？在什么场景下会用到呢？ PS：Keras是我最喜爱的深度学习框架了，其API的设计非常精妙和优雅，François Chollet是不愧是大师中的大师。相比传统的Tensorflow和PyTorch，Keras的API才是真正的“Deep Learning for Human”。另外，看到Tensorflow 2.0也开始以tf.keras作为第一公民，我非常欣慰。关于我对这几个框架的理解，后面再以专题文章和大家分享。 LSTM介绍 Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997) , and were refined and popularized by many people in following work.They work tremendously well on a large variety of problems, and are now widely used. LSTM是为了解决普通RNN网络在实际实践中出现的“梯度消失”等问题而出现的。这里我们略过里面的细节，重点看看单个LSTM cell的输入输出情况。从上图可以看出，单个LSTM cell其实有2个输出的，一个是h(t)，一个是c(t)。这里的h(t)称为hidden state，c(t)称为cell state。这个命名其实我认为是不太好的。熟悉全连接神经网络的同学，一定会把h(t)跟hidden layer相混淆。其实，这个h(t)才是LSTM的真正output，c(t)才是LSTM的内部”隐藏”状态。 我们进一步把LSTM网络展开来看。每一个时间节点timestep，输入一个x(t)，cell里面的c(t)做一次更新，输出h(t)。紧接着下一个timestep，x(t+1)、h(t)和c(t)继续输入到cell，输出为h(t+1)和c(t+1)，如下图。因此，Keras中的return_sequences和return_state，就是跟h(t)和c(t)相关。 Return Sequences接下来我们来点hands-on的代码，来具体看看这两个参数的作用。 实验一试验代码中，return_sequences和return_state默认都是false，输入shape为(1,3,1)，表示1个batch，3个timestep，1个feature 123456789101112from keras.models import Modelfrom keras.layers import Inputfrom keras.layers import LSTMfrom numpy import array# define modelinputs1 = Input(shape=(3, 1))lstm1 = LSTM(1)(inputs1)model = Model(inputs=inputs1, outputs=lstm1)# define input datadata = array([0.1, 0.2, 0.3]).reshape((1,3,1))# make and show predictionprint(model.predict(data)) 输出结果为 1[[-0.0953151]] 表示在经历了3个time step的输入后，LSTM返回的hidden state，也就是上文中的h(t)。由于输出的是网络最后一个timestep的值，因此结果是一个标量。 实验二我们加上参数return_sequences=True 1lstm1 = LSTM(1, return_sequences=True)(inputs1) 输出结果为 123[[[-0.02243521][-0.06210149][-0.11457888]]] 我们看到，输出了一个array，长度等于timestep，表示网络输出了每个timestep的h(t)。 总结一下，return_sequences即表示，LSTM的输出h(t)，是输出最后一个timestep的h(t)，还是把所有timestep的h(t)都输出出来。在实际应用中，关系到网络的应用场景是many-to-one还是many-to-many，非常重要。 Return State实验三接下来我们继续实验return_state 1lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1) 输出结果为 123[array([[ 0.10951342]], dtype=float32), array([[ 0.10951342]], dtype=float32), array([[ 0.24143776]], dtype=float32)] 注意，输出是一个列表list，分别表示 最后一个time step的hidden state 最后一个time step的hidden state（跟上面一样) 最后一个time step的cell state（注意就是上文中的c(t)） 可以看出，return_state就是控制LSTM中的c(t)输出与否。 实验四我们最后看看return_sequences和return_state全开的情况。 1lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True) 输出结果为 12345[array([[[-0.02145359], [-0.0540871 ], [-0.09228823]]], dtype=float32), array([[-0.09228823]], dtype=float32), array([[-0.19803026]], dtype=float32)] 输出列表的意义其实跟上面实验三一致，只是第一个hidden state h(t)变成了所有timestep的，因此也是长度等于timestep的array。 Time Distributed最后再讲一讲Keras中的TimeDistributed。这个也是在RNN中非常常用但比较难理解的概念，原作者解释说 TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor. 其实它的主要用途在于Many-to-Many： 比如输入shape为(1, 5, 1)，输出shape为(1, 5, 1) 123model = Sequential()model.add(LSTM(3, input_shape=(length, 1), return_sequences=True))model.add(TimeDistributed(Dense(1))) 根据上面解读，return_sequences=True，使得LSTM的输出为每个timestep的hidden state，shape为(1, 5, 3) 现在需要将这个(1 ,5, 3)的3D tensor变换为(1, 5, 1)的结果，需要3个Dense layer，分别作用于每个time step的输出。而使用了TimeDistributed后，则把一个相同的Dense layer去分别作用，可以使得网络更为紧凑，参数更少的作用。 如果是在many-to-one的情况，return_sequence=False，则LSTM的输出为最后一个time step的hidden state，shape为(1, 3)。此时加上一个Dense layer, 不用使用TimeDistributed，就可以将(1, 3)变换为(1, 1)。 总结本文主要通过一些实际的代码案例，解释了Keras的LSTM API中常见的两个参数return_sequence和return_state的原理及作用，在Tensorflow及PyTorch，也有相通的，希望能够帮助大家加深对RNN的理解。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy“手撕”朴素贝叶斯]]></title>
    <url>%2Fposts%2Fnumpy-naive-bayes%2F</url>
    <content type="text"><![CDATA[常见的python朴素贝叶斯算法的方式，都是使用for循环来统计各个p(特征|类型)的值。其实机器学习除了常规算法思路外，很关键且很优雅的地方在于矩阵化（向量化），即vectorization。通过各种矩阵运算来去除for循环，是目前机器学习、深度学习中非常关键的技巧。文本不使用各种高阶机器学习库，单纯使用numpy来手撕朴素贝叶斯算法，带你领略机器学习中的向量化之道。 数据准备为了demo方便，我们先伪造一些数据。这里伪造的是NLP情感分类的数据。分为两部门train和valid，标签分为1和0，1代表侮辱性文字，0代表正常言论。 12345678train = ['my dog had flea problems help help please', 'mybe not take him to dog park stupid dog', 'my dalmation is so cute I love him', 'stop posting stupid worthless garbage', 'mr licks ate my steak how to stop him',]valid = ['my stupid stupid worthless dog']label = [0,1,0,1,0]valid_y = [1] 数据预处理NLP的数据预处理主要就是词典的创建和文本的向量化。这里由于是英语文档，不存在分词的问题。 这里由于是手撕教程，不使用spacy等高阶NLP库，词典创建的也即使用Python的set 123456vocab = set([])for doc in train: vocab = vocab | set(doc.split())vocabList = list(vocab)# 根据词反查indexvocabList.index('please') 文档向量化向量话的原理，首先将文档中句子的每一个单词按照字典转换为index 123456train_vec = []valid_vec = []for doc in train: train_vec.append([vocabList.index(x) for x in doc.split()])for doc in valid: valid_vec.append([vocabList.index(x) for x in doc.split()]) 可以看看向量化后的结果 123456789train_vec, valid_vec'''[[28, 11, 16, 7, 26, 19, 19, 8], [20, 24, 21, 14, 6, 11, 12, 22, 11], [28, 5, 9, 25, 0, 3, 2, 14], [1, 17, 22, 15, 13], [18, 4, 27, 28, 10, 23, 6, 1, 14]], [[28, 22, 22, 15, 11]]''' 接着是将词典的每一个单词作为一个feature维度处理，将所有文档处理成相同维度的矩阵，维度大小即为词典的长度，而每个维度的值有多种处理方式，比如按照句子中每个单词的Count计数，或者是每个单词的TFIDF值。这里我们采用了Count，为了体现手撕，使用了Python的Counter，为了体现numpy矩阵运算的思路，使用了scipy的稀疏矩阵sparse.csr_matrix。 1234567891011121314151617from collections import Counterfrom scipy.sparse import csr_matrixdef get_term_doc_matrix(label_list, vocab_len): j_indices = [] indptr = [] values = [] indptr.append(0) for i, doc in enumerate(label_list): feature_counter = Counter(doc) j_indices.extend(feature_counter.keys()) values.extend(feature_counter.values()) indptr.append(len(j_indices)) return csr_matrix((values, j_indices, indptr), shape=(len(indptr) - 1, vocab_len), dtype=int)train_matrix = get_term_doc_matrix(train_vec, len(vocabList)valid_matrix = get_term_doc_matrix(valid_vec, len(vocabList) 可以看看矩阵化后的train和valid，分别转换成了samples*feature，29即为字典的维度。 1234567train_matrix, valid_matrix'''&lt;5x29 sparse matrix of type '' with 37 stored elements in Compressed Sparse Row format&gt;, &lt;1x29 sparse matrix of type '' with 4 stored elements in Compressed Sparse Row format&gt;''' 我们将生成的train_matrix作图如下（为方便观看省去了部分维度） 朴素贝叶斯算法原理谈到朴素贝叶斯算法，首先想到的必定是著名的贝叶斯公式 不过这个定理如何体现到我们机器学习的场景呢？我们把其中的A和B换成实际的物理意义如下： 在我们的分类监督学习中，其实就是求在一定特征的样本情况下，某个具体类别的概率。根据贝叶斯公式，可以转换成分别求某个类别下这些特征的概率、某类别的概率、特征的概率。 实际场景中，特征一般是多维的，而多维特征的联合概率是比较复杂的。这时候，就体现出朴素的概念了。我们对样本作出假设：样本中的各个特征是相互独立的。这样，可以将联合概率转换成以下的独立概率乘积： 当然，这种假设肯定是粗暴的。但是实践过程中，计算量减少带来的益处是远大于粗暴假设带来的失真。这也是最考验算法工程师的地方，既要考虑数学的严谨，同时也要考虑工程实现。“part-science, part-art”，这或许也是机器学习之道。 当然，对于模型最终的应用来说，求出具体的概率绝对值是意义不大的。我们只需要知道不同特征之间概率的比值就行。也就是说，对于二分类问题，我们只需要知道： 就可以判断出该样本是属于类别1。于是，我们可以接着进行公式变换，将P(类别2|特征)移到等式左边，并取log： 同样，根据朴素的假设，可以得到： 即，我们需要求出每个特征对应的 以及每个类别对应的 然后全部相加，看结果是否大于0即可判断出样本的类别。而这两块，我们都是可以从训练样本中求出（先验概率），这也就是朴素贝叶斯算法训练的部分。 ##朴素贝叶斯矩阵运算 前言中说到，一般的教程，直接使用for循环数数一般就可以求出上述的两个log值，完成“训练”。我们这样，要充分应用numpy的矩阵运算特性，不使用for循环 123456import numpy as npp1 = np.squeeze(np.asarray(train_matrix[np.asarray(label==1].sum(0)))p0 = np.squeeze(np.asarray(train_matrix[np.asarray(label==0].sum(0)))pr1 = (p1+1) / ((np.asarray(label)==1).sum() + 1)pr0 = (p0+1) / ((np.asarray(label)==0).sum() + 1)r = np.log(pr1/pr0) 还是以这个图为例，pr1即为类别为1的样本中特征x的概率。在NLP中，每个特征都是一个词，其物理意义就是某个词在某个类别中出现的概率，而最后求出的r即为所有特征的概率在2个类别的比值，注意，这里的r是个向量，是通过样本之间的矩阵运算一次性求出。 123456789r'''array([-0.40546511, 0.28768207, -0.40546511, -0.40546511,-0.40546511, -0.40546511, 0.28768207, -0.40546511, -0.40546511, -0.40546511, -0.40546511, 0.69314718, 0.98082925, 0.98082925, -0.11778304, 0.98082925, -0.40546511, 0.98082925, -0.40546511, -0.81093022, 0.98082925, 0.98082925, 1.38629436, -0.40546511, 0.98082925, -0.40546511, -0.40546511, -0.40546511, -1.09861229])''' 接下来求第二个log，这个直接就是不同类别的比例。 1234b = np.log((np.asarray(label)==1).mean() / (np.asarra(label)==0).mean())'''-0.4054651081081643''' 这里的r和b即为我们训练出的模型的参数。可以将其序列化到磁盘，供后续预测的时候使用（也就是深度学习中的inference）。 朴素贝叶斯推理由于r代表了训练样本中每一个特征词的在不同类别的概率比值，比如’stupid’等词就很高，而’love’等词就较低。在实际应用中，我们同样取出预测样本中的所有词，然后考虑这些词的概率比值，最后全部相加，即可判断最终样本的类别。同样，我们使用numpy的矩阵运算。 1234train_matrix@r + b &gt; 0# array([False, True, False, True, False])valid_matrix@r + b &gt; 0# array([ True]) 这里@是numpy的矩阵乘操作，举例如图而b作为一个标量，会直接触发numpy中的broadcast操作，直接把+应用到所有样本，同样&gt;也会broadcast，最终得到一个array，表示每一个测试样本是否预测为类别1。 扩展由于上面推理过程中，train_matrix和valid_matrix都是由词频组成，相当于在求具体概率比值的时候应用词频做了加权。在实际应用中，也可考虑不使用词频，直接使用是否含有某个特征词，如下： 12train_matrix.sign()@r + b &gt; 0valid_matrix.sign()@r + b &gt; 0 具体哪种方式好，还是要根据实际的数据样本情况来具体测试。 总结本文以NLP中的句子情感分析为例，使用numpy的矩阵运算来手撕朴素贝叶斯算法，可以加深对与朴素贝叶斯算法的理解，并体会机器学习中矩阵运算之道。 朴素贝叶斯简单快捷，特别适合较大规模的稀疏矩阵，在情感分析、垃圾邮件分类等场景中有很普遍的应用。在深度学习大行其道的今天，我们仍然可以通过这个简单的算法来快速搭建整个流程的pipeline。在我眼里，在机器学习项目的初期，流程pipeline的重要性大于任何算法。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我要开始写作？]]></title>
    <url>%2Fposts%2Fwhy-start-writting%2F</url>
    <content type="text"><![CDATA[源起– 说起博客，在大学阶段曾经玩过，当时玩的还是MSN Spaces。不过上面更多是一些经历、感悟，为赋新词强说愁罢了。后来读了研究生、有了家庭、有了事业，个人博客也就逐渐荒废了。现在MSN Spaces早已关闭，之前的文章也就无从寻觅了。工作以后，写作方面更多的是工作记录，以零散笔记为主，主要是用的Evernote以及现在的Bear，但是太过零散不成体系。 无意之中看到了Andrew Chen的一篇推文，“关于写作的15条推特”，深得我心，也是我为什么想在2019年开始写作的最好诠释。 Andrew Chen是在用户增长(User Growth)方面的专家，也曾任Uber的增长VP。以下是他的推文原文，并附上我的思考。 Andrew Chen’s TweetstormTweet #1 1/ After 10+ years of publishing professional writing at http://andrewchen.co , I have a couple opinions on how to get your stuff read. 在过去十年的工作生涯中，我有着非常多的经历，从软件工程师到Scrum Master，从数据分析到大数据挖掘，从机器学习到深度学习。而这些经历更多的是散落在各处，没有一个地方可以沉淀下来。当我意识到这是个很严重的问题时，我也开启了我自己的个人网站machinelearningzen.com的建设，希望能够汇聚我的经历和感悟，并把它们分享出来。 Tweet #2 2/ Titles are 80% of the work, but you write it as the very last thing. It has to be an compelling opinion or important learning 现在人的生活节奏都非常的快，这也是“标题党”横行的客观原因。但是我觉得除了有一个引人入胜的标题外，还是要有能够撑得起这个标题的干货内容才行。 Tweet #3 3/ There’s always room for high-quality thoughts/opinions. Venn diagram of people w/ knowledge and those we can communicate is tiny 这个也应证了“内容为王”。现在虽是自媒体时代，但是各种抄袭、伪原创仍然遍地都是。我希望我自己的内容都是自己经过深度思考后的输出，而不是东拼西凑。 Tweet #4 4/ Writing is the most scalable professional networking activity - stay home, don’t go to events/conferences, and just put ideas down 非常赞同，感谢互联网和移动互联网，我们可以呆在家里或者星巴克，一杯咖啡，就能跟全世界志同道合的朋友交流。 Tweet #5 5/ Think of your writing on the same timescale as your career. Write on a multi-decade timeframe. This means, don’t just pub on Quora/Medium 写作是一个非常好的把自己的碎片知识体系化、融会贯通的方式。以十年为单位日积月累，你的知识体系会像参天大树一样。 Tweet #6 6/ Focus on writing freq over anything else. Schedule it. Don’t worry about building an immediate audience. Focus on the intrinsic. 专注于内容本身，不用担心没有读者。另外，保持写作的频率，人都会有“拖延症”，有些事情，一旦开始拖就等于结束了。 Tweet #7 7/ To develop the habit, put a calendar reminder each Sunday for 2 hours. Forced myself to stare at a blank text box and put something down 养成习惯，必须要有很强的计划力和执行力。这些能力在工作当中同样重要，需要一辈子的修行。 Tweet #8 8/ Most of my writing comes from talking/reading deciding I strongly agree or disagree. These opinions become titles. Titles become essays. 保持阅读和交流非常重要。我的工作生涯的前几年，由于圈子太窄，接触的信息也很少，导致成长非常缓慢。后来注意到这个问题后，开始大量的阅读和交流，自己也开始了突飞猛进。现在，是时候把这些阅读和交流再继续沉淀成文字，给更多的人分享。 Tweet #9 9/ People are often obsessed with needing to write original ideas. Forget it. You’re a journalist with a day job in the tech industry 不用拘泥于开天辟地，现在的科技领域已经不是个人单打独斗的时代了。学会踩在巨人的肩膀上，你才能看得更远。 Tweet #10 10/ An email subscriber is worth 100x twitter or LinkedIn followers or whatever other stuff is out there. An email = a real channel Email 订阅在国外非常流行，可是在中国一直发展不起来，或许就是人们的使用习惯导致。好在移动互联网时代，微信公众号也是一个真正的推送渠道。我也开通了自己的微信公众号，文章和个人网站同步，希望喜欢听我的分享的朋友可以通过这种方式得到最新文章的推送。 Tweet #11 11/ I started writing while working at a VC. They asked, “Why give away ideas? That’s your edge.” Ironic that VCs blog/tweet all day now ;) 这个也是很多人不愿意写博客分享的原因，害怕自己的idea被别人抄袭。其实写文章分享idea，获得巨大提升的还是自己。 Tweet #12 12/ Publishing ideas, learnings, opinions, for years &amp; years is a great way to give. And you’ll figure out how to capture value later 年复一年地发表你的想法、学到的知识、观点，你很快就能在分享之后获得其中的价值。虽然我才开始真正的写作，我已经感受到了自己的成长，我会一直把它持续下去。 Tweet #13 13/ Today I learned that tweetstorms can be way less structured than writing an essay, which makes it much easier ;) This is my first one! Andrew Chen认为在twitter上写作是另外一种轻松愉快的方式，他把这种方式命名为“推特风暴” Tweet #14 14/ OK that’s all for now! Thanks for reading :) :) Tweet #15 15/ Bonus: In order of value, Writing &gt; Reading books &gt; Reading reddit &gt; Twitter &gt; FB/Instagram ;) 这个是Andrew认为的价值排序：写作 &gt; 读书 &gt; 读 reddit &gt; Twitter &gt; Facebook/Instagram。当然这是站在个人提升的价值上的，切记，我们的工作不等于我们的生活。保持自己1～2个爱好，并持续投入成为达人，你的人生会更加美好。]]></content>
      <categories>
        <category>感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy“手撕”文本主题模型之LSA]]></title>
    <url>%2Fposts%2Fnumpy-lsa%2F</url>
    <content type="text"><![CDATA[前言在前文中，我们采用了一个tf-idf来获取本文的关键词（主题）。由于tf-idf算法仅仅是一个统计模型，简单快速，适合作为baseline。它最大的问题在于单单考虑了词频，而没有考虑语义，并且我们取topK的词，这个K值如何选取，也是一个问题。 本文介绍一种真正的主题模型，也是最早出现的主题模型LSA（Latent Semantic Analysis）：潜在语义分析，它主要是利用SVD降维的方式，将词与本文映射到一个新的空间，而这个空间正是以主题作为维度。它的原理非常漂亮，一次奇异值分解就可以得到主题模型，同时也解决了词义的问题。 本文继续发挥hands-on的传统，以一个实例来说明LSA的用法。在生产环境中，一般会使用gensim等框架来快速进行开发，本文从scipy和numpy入手，可以更清楚的了解其中的原理。 数据预处理为了演示方便，我们直接采用了scikit-learn中的Newsgroups数据集。这是用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。数据集收集了大约20,000左右的新闻组文档，均匀分为20个不同主题的新闻组集合。 我们截取了其中4个主题的数据，并采用scikit-learn中的API来装载。 12345from sklearn.datasets import fetch_20newsgroupscategories = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']remove = ('headers', 'footers', 'quotes')newsgroups_train = fetch_20newsgroups(subset='train',categories=categories, remove=remove)newsgroups_test = fetch_20newsgroups(subset='test',categories=categories, remove=remove) 数据下载好后，我们看看里面的文本长啥样 12345678910111213141516171819newsgroups_train.data[:5]'''["Hi,\n\n I've noticed that if you only save a model (withall your mapping planes\n positioned carefully) to a .3DSfile that when you reload it after restarting\n 3DS, theyare given a default position and orientation. But if yousave\nto a .PRJ file their positions/orientation arepreserved. Does anyone\n know why this information is notstored in the .3DS file? Nothing is\nexplicitly said inthe manual about saving texture rules in the .PRJ file.\n I'd like to be able to read the texture rule information,does anyone have \n the format for the .PRJ file?\n\n Is the.CEL file format available from somewhere?\n\n Rych", '\n\n Seems to be, barring evidence to the contrary, that Koresh was simply\n another deranged fanatic who thought it neccessary to take a whole bunch of\n folks with him, children and all, to satisfy his delusional mania. Jim\n Jones, circa 1993.\n\n\n Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n for centuries.']''' 拿到文本后，第一件事当然是tokenizer，然后采用bag-of-words词袋模型将其向量化。这里使用scikit-learn中的CountVectorizer，以词频计数来作为向量值，当然更精细化可以采用TfidfVectorizer。 12345import numpy as npfrom sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer(stop_words='english')vectors = vectorizer.fit_transform(newsgroups_train.data.todense()vocab = np.array(vectorizer.get_feature_names()) 可以看看向量化后的矩阵 1234vectors.shape'''(2034, 26576)''' 这里表示共有2034篇文档，词的vocab_size为26576，将所有文档转成了26576维的向量，每个向量的值为该维表示词的词频。 SVD分解向量化完成后，我们就可以开始奇异值分解了。 12import scipyU, s, Vh = scipy.linalg.svd(vectors, full_matrices=False) 根据上图，我们可以看到分解出来的矩阵： U: (2034, 2034) 表示2034个样本，对应2034个topic s: (2034, ) 表示2034个奇异值，即topic的重要性分数 Vh: (2034, 26576) 表示2034个topic，对应26576个vocab，注意这个Vh是公式中的VT，即转置后的V SVD奇异值分解，具有以下性质： 奇异值分解为精确分解，即分解后的矩阵可以完全还原原矩阵，信息不丢失 U和Vh是正交矩阵 我们可以在numpy中验证一下： 1234# 注意将s从一维向量转换成对角矩阵(diagonal matrix)np.allclose(U.dot(np.diag(s)).dot(Vh), vectors)np.allclose(U.T.dot(U), np.eye(U.shape[0]))np.allclose(Vh.dot(Vh.T), np.eye(Vh.shape[0])) Topic解读LSA的优雅之处，就是把之前的高维文档向量，降维到低维，且这个维度代表了文档的隐含语义，即这个文档的主题topic。svd分解出来的Vh矩阵，即是每个主题的矩阵，维度是每个单词，维度值可以看成是这个主题中每个单词的的重要性。那么，我们可以选取重要性最高的词，来解读某个隐含主题。 123456789101112131415161718num_top_words = 8def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a])return [' '.join(t) for t in topic_words]show_topics(Vh[:10])'''['ditto critus propagandist surname galacticentrickindergarten surreal imaginative', 'jpeg gif file color quality image jfif format', 'graphics edu pub mail 128 3d ray ftp', 'jesus god matthew people atheists atheism does graphics', 'image data processing analysis software available tools display', 'god atheists atheism religious believe religion argument true', 'space nasa lunar mars probe moon missions probes', 'image probe surface lunar mars probes moon orbit', 'argument fallacy conclusion example true ad argumentum premises', 'space larson image theory universe physical nasa material']''' 可以看出，有些主题是关于图片格式的，有的是关于邮件协议的，有的是关于太空的。 而svd分解出来的U矩阵，就是每个文档对应的主题矩阵，维度是每个主题，维度值也可以看成是每个主题的重要性。 Truncated SVD在生产实践中，普通svd由于要exact decomposition，计算量会非常大，且最后的应用往往会只看前n个topic，因此Truncated SVD的优势在于，预先设置好n的值，可以在牺牲一定精度的条件下，大大减少计算量。这里提供两种常见的svd实现以作参考。 sklearn实现12from sklearn import decompositionu, s, v = decomposition.randomized_svd(vectors, 10) Facebook实现12import fbpcau, s, v = fbpca.pca(vectors, 10) 总结LSA作为最早出现的真正主题模型，非常优雅，同时也存在很多不足，比如它得到的不是一个概率模型，同时得到的向量中含有负值，难以直观解释。针对这些问题，后续就有NMF（非负矩阵分解），LDA（隐含狄利克雷分布）等模型出现。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>LSA</tag>
        <tag>SVD</tag>
        <tag>Topic Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你快速入门知识图谱 - Neo4J教程]]></title>
    <url>%2Fposts%2Fknowledge-graph-neo4j-toturial%2F</url>
    <content type="text"><![CDATA[前言今天，我们来聊一聊知识图谱中的Neo4J。首先，什么是知识图谱？先摘一段百度百科： 知识图谱（Knowledge Graph），在图书情报界称为知识域可视化或知识领域映射地图，是显示知识发展进程与结构关系的一系列各种不同的图形，用 可视化技术描述知识资源及其载体，挖掘、分析、 构建、绘制和显示知识及它们之间的相互联系。知识图谱是通过将应用数学、 图形学、信息可视化技术、 信息科学等学科的理论与方法与计量学引文分析、共现分析等方法结合，并利用可视化的图谱形象地展示学科的核心结构、发展历史、 前沿领域以及整体知识架构达到多学科融合目的的现代理论。它能为学科研究提供切实的、有价值的参考。 简单说来，知识图谱就是通过不同知识的关联性形成一个网状的知识结构，而这个知识结构，恰好就是人工智能AI的基石。当前AI领域热门的计算机图像、语音识别甚至是NLP，其实都是AI的感知能力，真正AI的认知能力，就要靠知识图谱。 知识图谱目前的应用主要在搜索、智能问答、推荐系统等方面。知识图谱的建设，一般包括数据获取、实体识别和关系抽取、数据存储、图谱应用都几个方面。本文着眼于数据存储这块，给大家一个Neo4J的快速教程。 Neo4J简介知识图谱由于其数据包含实体、属性、关系等，常见的关系型数据库诸如MySQL之类不能很好的体现数据的这些特点，因此知识图谱数据的存储一般是采用图数据库（Graph Databases）。而Neo4j是其中最为常见的图数据库。 Neo4J安装首先在 https://neo4j.com/download/ 下载Neo4J。Neo4J分为社区版和企业版，企业版在横向扩展、权限控制、运行性能、HA等方面都比社区版好，适合正式的生产环境，普通的学习和开发采用免费社区版就好。 在Mac或者Linux中，安装好jdk后，直接解压下载好的Neo4J包，运行bin/neo4j start即可 Neo4J使用Neo4J提供了一个用户友好的web界面，可以进行各项配置、写入、查询等操作，并且提供了可视化功能。类似ElasticSearch一样，我个人非常喜欢这种开箱即用的设计。 打开浏览器，输入http://127.0.0.1:7474/browser/，如下图所示，界面最上方就是交互的输入框。 Cypher查询语言Cypher是Neo4J的声明式图形查询语言，允许用户不必编写图形结构的遍历代码，就可以对图形数据进行高效的查询。Cypher的设计目的类似SQL，适合于开发者以及在数据库上做点对点模式（ad-hoc）查询的专业操作人员。其具备的能力包括： 创建、更新、删除节点和关系 通过模式匹配来查询和修改节点和关系 管理索引和约束等 Neo4J实战教程直接讲解Cypher的语法会非常枯燥，本文通过一个实际的案例来一步一步教你使用Cypher来操作Neo4J。 这个案例的节点主要包括人物和城市两类，人物和人物之间有朋友、夫妻等关系，人物和城市之间有出生地的关系。 1. 首先，我们删除数据库中以往的图，确保一个空白的环境进行操作： 1MATCH (n) DETACH DELETE n 这里，MATCH是匹配操作，而小括号()代表一个节点node（可理解为括号类似一个圆形），括号里面的n为标识符。 2. 接着，我们创建一个人物节点： 1CREATE (n:Person &#123;name:&apos;John&apos;&#125;) RETURN n CREATE是创建操作，Person是标签，代表节点的类型。花括号{}代表节点的属性，属性类似Python的字典。这条语句的含义就是创建一个标签为Person的节点，该节点具有一个name属性，属性值是John。 如图所示，在Neo4J的界面上可以看到创建成功的节点。3. 我们继续来创建更多的人物节点，并分别命名： 12345CREATE (n:Person &#123;name:&apos;Sally&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Steve&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Mike&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Liz&apos;&#125;) RETURN nCREATE (n:Person &#123;name:&apos;Shawn&apos;&#125;) RETURN n 如图所示，6个人物节点创建成功4. 接下来创建地区节点 12345CREATE (n:Location &#123;city:&apos;Miami&apos;, state:&apos;FL&apos;&#125;)CREATE (n:Location &#123;city:&apos;Boston&apos;, state:&apos;MA&apos;&#125;)CREATE (n:Location &#123;city:&apos;Lynn&apos;, state:&apos;MA&apos;&#125;)CREATE (n:Location &#123;city:&apos;Portland&apos;, state:&apos;ME&apos;&#125;)CREATE (n:Location &#123;city:&apos;San Francisco&apos;, state:&apos;CA&apos;&#125;) 可以看到，节点类型为Location，属性包括city和state。 如图所示，共有6个人物节点、5个地区节点，Neo4J贴心地使用不用的颜色来表示不同类型的节点。 5. 接下来创建关系 123MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Person &#123;name:&apos;Mike&apos;&#125;) MERGE (a)-[:FRIENDS]-&gt;(b) 这里的方括号[]即为关系，FRIENDS为关系的类型。注意这里的箭头--&gt;是有方向的，表示是从a到b的关系。 如图，Liz和Mike之间建立了FRIENDS关系，通过Neo4J的可视化很明显的可以看出： 6. 关系也可以增加属性 123MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Person &#123;name:&apos;Sally&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2001&#125;]-&gt;(b) 在关系中，同样的使用花括号{}来增加关系的属性，也是类似Python的字典，这里给FRIENDS关系增加了since属性，属性值为2001，表示他们建立朋友关系的时间。 7. 接下来增加更多的关系 1234MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Person &#123;name:&apos;John&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2012&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;), (b:Person &#123;name:&apos;Shawn&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2006&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Sally&apos;&#125;), (b:Person &#123;name:&apos;Steve&apos;&#125;) MERGE (a)-[:FRIENDS &#123;since:2006&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Person &#123;name:&apos;John&apos;&#125;) MERGE (a)-[:MARRIED &#123;since:1998&#125;]-&gt;(b) 如图，人物关系图已建立好，有点图谱的意思了吧？ 8. 然后，我们需要建立不同类型节点之间的关系-人物和地点的关系 1MATCH (a:Person &#123;name:&apos;John&apos;&#125;), (b:Location &#123;city:&apos;Boston&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1978&#125;]-&gt;(b) 这里的关系是BORN_IN，表示出生地，同样有一个属性，表示出生年份。 如图，在人物节点和地区节点之间，人物出生地关系已建立好。 9. 同样建立更多人的出生地 1234MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;), (b:Location &#123;city:&apos;Boston&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1981&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;), (b:Location &#123;city:&apos;San Francisco&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1960&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;), (b:Location &#123;city:&apos;Miami&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1960&#125;]-&gt;(b)MATCH (a:Person &#123;name:&apos;Steve&apos;&#125;), (b:Location &#123;city:&apos;Lynn&apos;&#125;) MERGE (a)-[:BORN_IN &#123;year:1970&#125;]-&gt;(b) 建好以后，整个图如下 10. 至此，知识图谱的数据已经插入完毕，可以开始做查询了。我们查询下所有在Boston出生的人物 1MATCH (a:Person)-[:BORN_IN]-&gt;(b:Location &#123;city:&apos;Boston&apos;&#125;) RETURN a,b 结果如图 11. 查询所有对外有关系的节点 1MATCH (a)--&gt;() RETURN a 注意这里箭头的方向，返回结果不含任何地区节点，因为地区并没有指向其他节点（只是被指向） 12. 查询所有有关系的节点 1MATCH (a)--() RETURN a 结果如图 13. 查询所有对外有关系的节点，以及关系类型 1MATCH (a)-[r]-&gt;() RETURN a.name, type(r) 结果如图 14. 查询所有有结婚关系的节点 1MATCH (n)-[:MARRIED]-() RETURN n 结果如图 15. 创建节点的时候就建好关系 1CREATE (a:Person &#123;name:&apos;Todd&apos;&#125;)-[r:FRIENDS]-&gt;(b:Person &#123;name:&apos;Carlos&apos;&#125;) 结果如图 16. 查找某人的朋友的朋友 1MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;)-[r1:FRIENDS]-()-[r2:FRIENDS]-(friend_of_a_friend) RETURN friend_of_a_friend.name AS fofName 返回Mike的朋友的朋友：从图上也可以看出，Mike的朋友是Shawn，Shawn的朋友是John和Sally 17. 增加/修改节点的属性 1234MATCH (a:Person &#123;name:&apos;Liz&apos;&#125;) SET a.age=34MATCH (a:Person &#123;name:&apos;Shawn&apos;&#125;) SET a.age=32MATCH (a:Person &#123;name:&apos;John&apos;&#125;) SET a.age=44MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) SET a.age=25 这里，SET表示修改操作 18. 删除节点的属性 12MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) SET a.test=&apos;test&apos;MATCH (a:Person &#123;name:&apos;Mike&apos;&#125;) REMOVE a.test 删除属性操作主要通过REMOVE 19. 删除节点 1MATCH (a:Location &#123;city:&apos;Portland&apos;&#125;) DELETE a 删除节点操作是DELETE 20. 删除有关系的节点 1MATCH (a:Person &#123;name:&apos;Todd&apos;&#125;)-[rel]-(b:Person) DELETE a,b,rel 总结本文重点针对常见的知识图谱图数据库Neo4J进行了介绍，并且采用一个实际的案例来说明Neo4J的查询语言Cypher的使用方法。 当然，类似MySQL一样，在实际的生产应用中，除了简单的查询操作会在Neo4J的web页面进行外，一般还是使用Python、Java等的driver来在程序中实现。后续会继续介绍编程语言如何操作Neo4J。]]></content>
      <categories>
        <category>Knowledge Graph</category>
      </categories>
      <tags>
        <tag>Neo4J</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代NLP的基石 - Word2Vec]]></title>
    <url>%2Fposts%2Fnlp-word2vec%2F</url>
    <content type="text"><![CDATA[起源机器学习的各项算法，都可以简单理解为：一个向量vector，经过一个函数f(x)，结果得到另一个向量vector或者标量scalar。输入的这个向量，我们称之为样本sample，向量中的每一个值，代表了样本的每一维特征feature。这个向量一般是浮点数的，即dtype=np.float32。 在NLP的相关领域中，我们的样本往往是一段文本内容，如一个句子、一篇文章等等，我们不能把这些文本直接喂给机器学习模型，而是要先把文本转换成浮点数的向量，即文本的向量化表示。这在NLP领域中是非常基础且重要的阶段，其结果可以用在文档搜索，网页搜索，垃圾邮件过滤，文本主题建模等等方面。 BOW在文本的向量话表示领域中，最早出现的方法是BOW（Bag Of Words），即词袋模型。这个模型利用onehot编码的思想，把每一个词看作是一个维度，然后一个文档或句子就可以表示成一个维度为N的向量，N为语料中词语的个数。每个维度具体的值，可以通过简单计数或者计算TF-IDF值来得出。实际方案可参考Scikit Learn的CountVectorizer和TfidfVectorizer。 这种方式简单，但是效果一般，因为它忽略了很多信息，比如在一个句子中词语的顺序。另外，由于每个词语都是独立的维度，词与词之间的距离（相关）都是相等的，这也明显不合理。 Word2VecWord2vec 1301.3781 Efficient Estimation of Word Representations in Vector Space是这个领域非常重要的理念。我认为，Word2Vec是NLP迈向深度学习的基石，通过embedding的方法，让模型能够理解到每个词背后的隐藏含义。 Word2Vec认为，每一个词都可以表示为一个密集多维向量（相比BOW中的稀疏向量），且这个向量可以捕捉词与词之间的关联。显然，密集的浮点向量，对于机器学习模型是非常友好的。 Word2Vec除了采用密集浮点向量来表示每一个单词外，还能实现一个特殊的功能，把数学运算和文本结合起来，比如著名的“king-man-queen-woman”图如下所示： 采用Word2Vec表示以后，我们可以通过矩阵运算，得出 king-man = queen-woman的等式，即词语的向量中，包含了king和queen中关于王权的概念，同时也包含了man和woman的男女之别。 Word2Vec按照实现算法，可分为两类： CBOWCBOW(Continuous Bag Of Words)是实现word2vec的算法之一。它在每个词语上构造了一个滑动窗口，即词语的上下文-词周围的那些词。每个词都用一个固定长度的向量来表示，然后就可以根据词周围的词来预测中间那个词。 在训练过程中，根据每次的预测情况，使用GradientDesent方法，不断调整周围词的向量，当训练完成以后，每个词都会作为中心词，把周围的词向量进行了调整，而这种调整是统一的，即求出的gradient值会同样作用到每个周围词的词向量中。 值得注意的是，在实际操作过程中，周围词的向量会首先通过sum或者concat的方式组合在一起，然后进行预测，在训练过程中，每个epoch计算的次数跟整个文本的词数几乎相等，复杂度大概为O(V)。 更通俗一点的理解，相当于1个老师对应了K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家一样的知识。因此，这种方式对于具体某个学生，学习的结果不一定是最好的，但是整个班级的训练过程是效率最高最快的。 Skip-gramSkip-gram是word2vec的另一种实现算法。它跟cbow完全相反，即使用一个词来预测它周围的词。在skip-gram中，会利用对周围词的预测情况，使用GradientDecent来不断的调整中心词的词向量，最终所有文本遍历完以后，也就得到了文本所有词的词向量。 可以看出，skip-gram的训练次数是要多于cbow的，因为每个词在作为中心词时，都需要把它周围的词全部预测一次，这样相当于比cbow的方法多进行了K-1次（假设K为窗口大小），因此复杂度为O(KV)。 如图所示，这里的窗口值为2，蓝色的词是输入，窗口内的其他词为输出，这样将一个句子转化为训练样本。 在skip-gram当中，每个词都要受到周围词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整，因此，当数据量较少的时候，或者词为生僻词出现次数较少时，这种多次的调整会使得词向量更加的准确。 对比上面的老师与学生的比喻，在skip-gram中，每个词作为中心词时，其实是1个学生 对应了K个老师，K个老师（周围词）都会对学生（中心词）进行训练，这样单个学生的能力就会相对扎实一些，但是这种训练方式整个班级的学习时间肯定更长。 Negative Sample按照正常的训练逻辑，cbow和skip-gram都是分类模型，分类类别数等于语料中词语的个数。因此模型在最后的softmax层非常巨大，会导致训练的时间非常长，且需要非常大的训练数据来收敛模型和避免over-fitting。 举个例子，若有一个包含10000个单词的词汇表，而向量特征为300维，则在最后一层Dense层会有300x10000个权重需要更新。 为了解决这个问题，word2vec的作者提出了2个方案： Subsampling frequent words：以一定的阈值过滤掉了一些高频词，比如“the”，这样可以在一定程度上减少训练数据的数量。 Negative sampling：改变优化目标函数，使得每一个训练样本只更新网络的小部分权重。 举个例子，在训练样本(”fox”, “quick”)时，我们的输出（标签）为一个one-hot向量，且代表”quick”的神经元输出为1，代表其他成千上万的词的神经元输出为0。采用了negative sampling后，我们首先随机选取一小部分”negative”词语（比如5个）去更新网络的权重，而”positive”词语依然也会更新网络权重。如果在网络的输出层，以前是300x10000的矩阵，现在只需要更新1+5=6个单词，即300x6个权重，是以前的0.06%。在Tensorflow中，有现成的辅助函数tf.nn.nce_loss()可供使用。而在具体的negative samples的选取上，使用了“unigram distribution”，即更频繁使用的词语更容易被选取。 总结本文介绍了Word2Vec的起源及相应实现的2种算法cbow和skip-gram，详细对比了这两种算法的原理、训练方式及优缺点，还介绍了Word2Vec训练时的重要trick: negative sampling。Word2Vec虽然不算深度学习，但是它带来的embedding的概念却是现代NLP乃至深度学习的基石，后续我们会继续介绍由此发展的Doc2Vec、Item2Vec、Anything2Vec。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任意网页正文内容主题词提取]]></title>
    <url>%2Fposts%2Fweb-topic-extractor%2F</url>
    <content type="text"><![CDATA[前言网页正文内容主题提取，即任意给一个网页url，通过爬取网页内容和文本分析，得出该网页内容的关键词，作为网页的标签。这些关键词和标签在做流量分析_内容推荐方面有非常重要的意义。比如说我们做数字营销，用过页面来做用户引流，我们就可以知道吸引用户过来的点是什么，用户的潜在需求是什么；另外，针对内容社区的用户画像/推荐系统，关键点也是文章/页面的主题和标签。 这个任务涉及的技术点主要有以下几个： 网页爬虫。做网页内容分析，首先得根据url把网页内容扒下来吧。 正文提取。现在的web页面是非常复杂的，除了正文外，包含了大量的广告、导航、信息流等，我们需要去除干扰，只提取网页的正文信息。 主题模型 。拿到正文文本后，就需要做NLP来提取主题关键字了。 网页爬虫这里的网页爬虫和一般的爬虫还不太一样，会简单许多，主要是把原始网页的HTML抓下来即可，主要是为后续的分析挖掘打下基础，属于数据采集的阶段。 这里我们采用了Python的requests包。requests相对于Python自带的urllib来说，API更为人性化，鲁棒性也更好。 1234import requestsr = request.get(url)r.encoding='utf-8'html = r.text 正文提取通过研究爬取下来的原始HTML，我们可以看到是非常负责而且杂乱无章的，充斥着大量的js代码等。我们首先需要解析HTML，尽量过滤掉js代码，留下文本内容。 这里我们采用了Python的BeautifulSoup包。这个包堪称Python一大神器，解析HTML效果非常好 12345from bs4 import BeautifulSoupsoup = BeautifulSoup(html, features="html.parser")for script in soup(["script", "style"]): script.decompose()text = soup.get_text() 我们想要的是网页的正文内容，其他的诸如广告或者导航栏等干扰内容需要尽可能的过滤掉。通过BeautifulSoup可以解析出整个HTML的DOM树结构，但是每个网页HTML的写法各不相同，单纯靠HTML解析无法做到通用，因此我们需要跳出HTML的思维，使用其他的方法来提取网页的正文。这里有个很优雅的方式是“基于行块分布函数”的算法cx-extractor。 基于行块分布函数的通用网页正文抽取：线性时间、不建DOM树、与HTML标签无关对于Web信息检索来说，网页正文抽取是后续处理的关键。虽然使用正则表达式可以准确的抽取某一固定格式的页面，但面对形形色色的HTML，使用规则处理难免捉襟见肘。能不能高效、准确的将一个页面的正文抽取出来，并做到在大规模网页范围内通用，这是一个直接关系上层应用的难题。作者 提出了 《基于行块分布函数的通用网页正文抽取算法》 ，首次将网页正文抽取问题转化为求页面的行块分布函数，这种方法不用建立Dom树，不被病态HTML所累（事实上与HTML标签完全无关）。通过在线性时间内建立的行块分布函数图，直接准确定位网页正文。同时采用了统计与规则相结合的方法来处理通用性问题。作者相信简单的事情总应该用最简单的办法来解决这一亘古不变的道理。整个算法实现代码不足百行。但量不在多，在法。 上图就是某个页面求出的行块分布函数曲线。该网页的正文区域为145行至182行，即分布函数图上含有最值且连续的一个区域，这个区域往往含有一个骤升点和一个骤降点，因此，网页正文抽取问题转化为了求行块分布函数上的骤升点和骤降点两个边节点。 这里我们采用了这个算法的Python实现GitHub - chrislinan/cx-extractor-python： 1234from CxExtractor import CxExtractorcx = CxExtractor(threshold=40)text = cx.getText(text)texts = text.split('\n') 主题模型拿到网页正文内容文本后，就需要提取正文主题关键词了。常见做法有3种： TFIDF Text-Rank LSI/LDA 这里我们先采用TFIDF的方式来做。 TFIDF(Term Frequency Inverse Document Frequency)是一种用于信息检索与数据挖掘的常用加权技术。 词频（TF）=某个词在文本中出现的次数/该文本中总词数 逆向文档频（IDF）=log(语料库中所有文档总数/(包含某词的文档数+1)) 我们通过TF，也就是某个词在文本中出现的频度，来提升这个词在主题中的权重，然后我们通过IDF值，即逆向文档频来降低公共词的主题权重。TF*IDF也就得到了我们要的主题词权重。 做TFIDF，首先步骤是分词。分词的效果取决于词典的构建，且对后续关键词提取影响巨大。首先要基于分析的行业主题建立专用词典，然后还需要维护停用词的词典。有了词典后，我们就可以采用Python分词的神器jieba来处理分词。 12345678import jiebajieba.load_userdict('./dict.txt') #自定义词典stopwords = set([line.strip() for line in open('stopwords.txt', 'r', encoding='utf-8').readlines()]) #停用词典 word_lists = []for text in texts: word_lists += (list(jieba.cut(text, cut_all=False)))word_lists = [w for w in word_lists if not is_stop_word(w)] 分词完毕后，我们就可以计算TFIDF了。可以通过gensim，scikit-learn等机器学习专用包来做，jieba本身也提供这个功能，这里我们直接用jieba。 12import jieba.analysekeywords = jieba.analyse.extract_tags(' '.join(word_lists), topK=20, withWeight=True, allowPOS=['n', 'ns', 'nr', 'nt', 'nz']) 注意这里有个参数是allowPOS，按照词性过滤。这个需要根据实际的业务需求来设置。 词性标注(Part-Of-Speech Tagging, POS tagging)，是语料库语言学中将语料库内单词的词性按照其含义和上下文内容进行标记的文本数据处理技术。常见标注示例：n 名词nr 人名ns 地名nt 机构团体nz 其他专名a 形容词v 动词 服务到这里，我们的关键词提取就结束了，为了方便其他同学来使用，我们可以用Flask做一个restful api，输入为网址url，输出为提取出的关键词并排序。 总结在这篇文章里，我们完成了从任意网页url提取正文主题关键词的功能。在主题模型这块采用了常见的TFIDF的算法来解决，可以快速出一个原型提供给业务方使用。后续我们会继续优化，采用更多的算法来进一步提升效果。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Topic Model</tag>
        <tag>TFIDF</tag>
      </tags>
  </entry>
</search>
