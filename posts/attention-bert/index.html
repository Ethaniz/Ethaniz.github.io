<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="前言  BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representati">
<meta name="keywords" content="BERT">
<meta property="og:type" content="article">
<meta property="og:title" content="透过Attention看BERT">
<meta property="og:url" content="https://www.aifun.xyz/posts/attention-bert/index.html">
<meta property="og:site_name" content="机器拾趣">
<meta property="og:description" content="前言  BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representati">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.loli.net/2020/01/09/OlHPj9th32nayus.jpg">
<meta property="og:image" content="https://i.loli.net/2020/01/08/XRoUzY8gNJeuWwr.jpg">
<meta property="og:image" content="https://i.loli.net/2020/01/08/g4WqHGrs5A2VYdT.jpg">
<meta property="og:image" content="https://i.loli.net/2020/01/08/1i3MqKhQY9jkpVm.jpg">
<meta property="og:image" content="https://i.loli.net/2020/01/08/fBSzRkKvqwris2U.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/xUHXFhd3EcG8ivu.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/5kGipWxSHls2vwe.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/emAokFDLyd6Sgq7.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/7Kz8OMg3Dh2vxXI.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/4huVBRMlvKjpreU.png">
<meta property="og:image" content="https://i.loli.net/2020/01/08/9OYodBSAI3LDuPp.png">
<meta property="og:image" content="https://i.loli.net/2020/01/09/FEp9lVe3JsIDujn.png">
<meta property="og:image" content="https://i.loli.net/2020/01/09/OXrT3GNdq6upwht.png">
<meta property="og:updated_time" content="2020-01-08T16:18:15.709Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="透过Attention看BERT">
<meta name="twitter:description" content="前言  BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representati">
<meta name="twitter:image" content="https://i.loli.net/2020/01/09/OlHPj9th32nayus.jpg">
  <link rel="canonical" href="https://www.aifun.xyz/posts/attention-bert/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>透过Attention看BERT | 机器拾趣</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-147618128-2"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-147618128-2');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">机器拾趣</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">冰冷机器如何理解世事无常</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://www.aifun.xyz/posts/attention-bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="异尘">
      <meta itemprop="description" content="现居成都，多年大数据与机器学习经验，目前任某互联网金融公司算法专家">
      <meta itemprop="image" content="https://i.loli.net/2019/09/26/anrCA26pmV3R94F.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="机器拾趣">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">透过Attention看BERT

          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-01-09 00:01:54" itemprop="dateCreated datePublished" datetime="2020-01-09T00:01:54+08:00">2020-01-09</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://i.loli.net/2020/01/09/OlHPj9th32nayus.jpg" alt="0_q2OIJrDCRc1t90Dy.jpeg"></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p> BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
</blockquote>
<a id="more"></a>

<p>我相信，从18年到19年，在NLP领域中，出现频率最高的词就是BERT了。各种blog的介绍，各种公众号文章的分析，以及github上数不清的repo。BERT的出现，在自然语言处理方向上是划时代的。虽然后续相继出现了非常多的 RoBERTa 、ALBERT、 NEZHA等等模型，都是基于BERT上的改进，其根基都是BERT，或者严格意义上是Transformer。</p>
<p><img src="https://i.loli.net/2020/01/08/XRoUzY8gNJeuWwr.jpg" alt="v2-f51a6525b127fe6d89d57773c4dd26c6_1200x500.png"></p>
<p>今天，我们不做BERT的科普或者详解，而是站在另一个角度，透过<code>attention</code>来看BERT。</p>
<h2 id="BERT中的attention"><a href="#BERT中的attention" class="headerlink" title="BERT中的attention"></a>BERT中的attention</h2><p>attention其实很早就出现了。在BERT出现之前，attention机制就在seq2seq场景中大放异彩。当时的attention，主要是在decoding阶段对input中的信息赋予不同的权重。比如在做机器翻译的场景，针对翻译的每个字，原文中的每个字的权重都是不一样的。</p>
<p><img src="https://i.loli.net/2020/01/08/g4WqHGrs5A2VYdT.jpg" alt="v2-99839119705a82a807409d104f63e88a_hd.jpg"></p>
<p>而作为BERT的主要组成部分-Transformer中，提出了“Attention is all you need”的口号，全面应用了<code>Multi-Head Self Attention</code>。</p>
<p><img src="https://i.loli.net/2020/01/08/1i3MqKhQY9jkpVm.jpg" alt="v2-df2ca1b7a60d829245b7b7c37f80a3aa_hd.jpg"></p>
<p>这里的关键点有两个：</p>
<ol>
<li><code>Self-Attention</code>: 跟传统的encoder-decoder架构中的attention不一样，Transformer中的attention，主要是自注意力，也就是单个sequence里自己和自己的attention。这样就可以成功将一个sequence通过矩阵变换，抽取特征。</li>
<li><code>Multi-Head Attention</code>: 在Transformer中，非常类似CNN中多个卷积Filter的概念，采用多头Attention（BERT中是12个head），这样不同的HEAD，可以从不同的角度来抽取特征，使得特征信息更加丰富。</li>
</ol>
<p>接下来，让我们依照一贯的传统，根据实例来深入BERT的attention。</p>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>数据集采用大众点评的用户评论情感数据，如图：</p>
<p><img src="https://i.loli.net/2020/01/08/fBSzRkKvqwris2U.png" alt="1577762298330.png"></p>
<p>这是典型的文本二分类问题，也是BERT最为拿手的场景之一。</p>
<p><img src="https://i.loli.net/2020/01/08/xUHXFhd3EcG8ivu.png" alt="1577776392358.png"></p>
<p>将原始句子Tokenize后，输入BERT，经过多层的Transformer变换，最后取第一个<code>[CLS]</code>token的hidden state，然后接传统的Dense网络来输出。</p>
<h2 id="BERT框架"><a href="#BERT框架" class="headerlink" title="BERT框架"></a>BERT框架</h2><p>Google自身是开源了BERT代码的。但是说实话，站在软件工程的角度来看，google原版的BERT代码“很不专业”，可读性、扩展性都非常的差。这里我们采用HuggingFaces的Transformers，是我认为写的很好的BERT框架之一。</p>
<p><img src="https://i.loli.net/2020/01/08/5kGipWxSHls2vwe.png" alt="1577762594256.png"></p>
<p>用这个框架玩BERT模型，一般就是几板斧搞定，非常清晰：</p>
<ol>
<li><p>从预训练权重初始化Tokenizer，将原始文本转化成input_ids、attention_masks。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">input_ids = [tokenizer.encode(sent, add_special_tokens=<span class="literal">True</span>, max_length=MAX_LEN, pad_to_max_length=<span class="literal">True</span>) <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</span><br><span class="line">attention_masks = [[float(i&gt;<span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> seq] <span class="keyword">for</span> seq <span class="keyword">in</span> input_ids]</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>从预训练权重初始化BERT模型</li>
</ol>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-chinese"</span>, num_labels=<span class="number">2</span>, output_attentions=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>这里的关键点在于<code>output_attentions=True</code>这个参数。Transforms里面的模型，输出都是一个<code>tuple</code>，如图，分别是模型最后一层的各个hidden_state、<code>[CLS]</code>token的向量表示、每一层的hidden_state（需参数）、每一层的attention（需参数）。</p>
<p><img src="https://i.loli.net/2020/01/08/emAokFDLyd6Sgq7.png" alt="1577762940276.png"></p>
<ol start="3">
<li>初始化模型之后，就是常规的分Epoch/Batch来训练了，过程这里不再赘述。</li>
</ol>
<h2 id="Attention-Weights"><a href="#Attention-Weights" class="headerlink" title="Attention Weights"></a>Attention Weights</h2><p>这里我们拿出一个句子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">'超级差评\xa0\xa0上菜慢\xa0服务态度超级差、先把汤上来喝饱了\xa0完事菜就上了一个，找服务员催半天也上不来最后结账态度超级差一个个子不高在门口附近桌子的服务员'</span></span><br></pre></td></tr></table></figure>

<p>首先通过tokenizer来把文本数字化<code>input_ids</code>，并得到相应的<code>token_type_ids</code>和<code>attention_mask</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoded = tokenizer.encode_plus(test)</span><br><span class="line">input_ids = encoded[<span class="string">'input_ids'</span>]</span><br><span class="line">token_type_ids = encoded[<span class="string">'token_type_ids'</span>]</span><br><span class="line">attention_mask = encoded[<span class="string">'attention_mask'</span>]</span><br></pre></td></tr></table></figure>

<p>然后将它输入到训练好的BERT模型中，从输出拿到attention weights。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = model.bert(torch.tensor(input_ids).reshape(<span class="number">1</span>, <span class="number">-1</span>).detach().to(<span class="string">'cuda'</span>), token_type_ids=torch.tensor(token_type_ids).reshape(<span class="number">1</span>, <span class="number">-1</span>).detach().to(<span class="string">'cuda'</span>))</span><br><span class="line">attention = out[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>注意这里的输入需要用<code>detach</code>来从当前的计算图中分离开，不需要梯度，当一个data使。</p>
<p>这里得到的<code>attention</code>是一个列表，长度是12，表示BERT模型的12层每一层的attention weights。</p>
<p>这里的12层，跟多层CNN模型类似，每一层都提取出了不同粒度的特征（如经典CNN网络做猫狗识别中，底层提取出了点和边的特征，中间层提取出了眼睛鼻子耳朵特征，高层再抽象出猫和狗）。BERT处理自然语言一样，底层是单个字的特征（最下面是每个字的word embedding），然后到词，然后是一些语义，最后到句子。可以说，BERT让自然语言处理真正到达了“深度学习“的时代。</p>
<p>然后我们看看每一层的<code>attention</code>啥样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention[<span class="number">-1</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([1, 12, 71, 71])</span></span><br></pre></td></tr></table></figure>

<p>这里的第1维是batch_size，第2维表示multi-head中的12个head，后面的71x71就是我们每一个token的两两attention矩阵了。</p>
<h2 id="attention-weights可视化"><a href="#attention-weights可视化" class="headerlink" title="attention weights可视化"></a>attention weights可视化</h2><p>接下来就是把我们的attention weight可视化了。</p>
<p>首先，我们取倒数第2层的attention。为什么要倒数第2层呢？因为最后一层其实已经趋向于模型最终的二分类结果，不利于去观察每个token的情况。我非常欣赏的<a href="https://github.com/hanxiao" target="_blank" rel="noopener">hanxiao</a>博士，在他的大作<code>bert-as-service</code>中，也是取得倒数第二层的hidden state作为特征提取：</p>
<p><img src="https://i.loli.net/2020/01/08/7Kz8OMg3Dh2vxXI.png" alt="1577772281012.png"></p>
<p>我们的目标，是探寻每个字对于情感分析的影响，也就是说，第1个<code>[CLS]</code>token和句子中的每一个字的attention权重：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">att_matrix = attention[<span class="number">-2</span>].squeeze(<span class="number">0</span>)[:, <span class="number">0</span>, <span class="number">1</span>:<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>

<p>这里-2表示取倒数第2层，<code>squeeze(0)</code>取出batch，第1个<code>:</code>取出所有的head，<code>0</code>取出<code>[CLS]</code>，<code>1:-1</code>表示去掉句子中<code>[CLS]</code>和<code>[SEP]</code>的attention。最终，我们得到一个shape为(12, 69)的矩阵。</p>
<p>为了方便attention矩阵可视化观察，我们再将之归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">scaled = scaler.fit_transform(attention[<span class="number">-2</span>].squeeze(<span class="number">0</span>)[:, <span class="number">0</span>, <span class="number">1</span>:<span class="number">-1</span>].cpu().detach().numpy())</span><br></pre></td></tr></table></figure>

<p>然后，就是拿出万能的excel了：</p>
<p><img src="https://i.loli.net/2020/01/08/4huVBRMlvKjpreU.png" alt="1577772976993.png"></p>
<p><img src="https://i.loli.net/2020/01/08/9OYodBSAI3LDuPp.png" alt="1577772884214.png"></p>
<p><img src="https://i.loli.net/2020/01/09/FEp9lVe3JsIDujn.png" alt="1577772910692.png"></p>
<p><img src="https://i.loli.net/2020/01/09/OXrT3GNdq6upwht.png" alt="1577772936612.png"></p>
<p>可以看到，我们的“多头猛兽”每一个头的关注点都不太一样。有的头关注了“超级差评”、“超级差”等字眼，有的头关注了“服务态度”，有的头关注了“催”等等。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可以说，正是因为BERT中Transformer的multi-head“多头”，再加上多个Transformer叠加的多层，如此大的容量容纳了非常多的信息，一方面使得它在理解文字语义方面优势明显，另一方面，也使得“预训练”称为可能。当然，大容量也导致了这个“多头猛兽”是难以驯服的，各个大厂也一度开始“军备竞赛”，模型越来越大，消耗的算力也越来越猛。当然，我们也非常欣慰的看到很多人也在研究如何把BERT简化，让它去适合更多的实际生产场景，如Google自家的<code>ALBERT</code>、HuggingFace的<code>DistillBERT</code>、华为的<code>TinyBERT</code>等。</p>
<p>我一直认为，基于预训练的迁移学习，是深度学习技术相比传统机器学习最大的优势，也是非常类似人类去学习的一个过程。</p>
<p>要知道，ImageNet以及各种CNN预训练模型的百花齐放，使得深度学习在计算机视觉领域吊打一切；而BERT的横空出世， fast.ai创始人<a href="https://ruder.io/nlp-imagenet/" target="_blank" rel="noopener">Sebastian Ruder</a>非常兴奋的说：</p>
<blockquote>
<p> <a href="https://ruder.io/nlp-imagenet/" target="_blank" rel="noopener">NLP’s ImageNet moment has arrived</a> </p>
</blockquote>

    </div>

    
    
    
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center;">
  <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="异尘 wechat" style="width: 200px; max-width: 100%;">
  <div>欢迎您扫一扫上面的微信公众号，订阅我的博客</div>
</div>

      
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/BERT/" rel="tag"><i class="fa fa-tag"></i> BERT</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/posts/neo4j-lookalike-part2/" rel="next" title="教你用neo4j做lookalike（下）">
                  <i class="fa fa-chevron-left"></i> 教你用neo4j做lookalike（下）
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT中的attention"><span class="nav-number">2.</span> <span class="nav-text">BERT中的attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#场景"><span class="nav-number">3.</span> <span class="nav-text">场景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT框架"><span class="nav-number">4.</span> <span class="nav-text">BERT框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-Weights"><span class="nav-number">5.</span> <span class="nav-text">Attention Weights</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention-weights可视化"><span class="nav-number">6.</span> <span class="nav-text">attention weights可视化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://i.loli.net/2019/09/26/anrCA26pmV3R94F.png"
      alt="异尘">
  <p class="site-author-name" itemprop="name">异尘</p>
  <div class="site-description" itemprop="description">现居成都，多年大数据与机器学习经验，目前任某互联网金融公司算法专家</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://weibo.com/aifunxyz" title="Weibo &rarr; https://weibo.com/aifunxyz" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/yi-chen-25-73" title="知乎 &rarr; https://www.zhihu.com/people/yi-chen-25-73" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-laptop"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">异尘</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.0"></script>














  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '339ef61c48976b95fd3b',
      clientSecret: '2b1314629cd3ff31ce9ed5388c5d6e72c4465950',
      repo: 'ethaniz.github.io',
      owner: 'ethaniz',
      admin: ['ethaniz'],
      id: '323101e538294a4ff3ed96d052fc7766',
        language: window.navigator.language || window.navigator.userLanguage,
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

</body>
</html>
