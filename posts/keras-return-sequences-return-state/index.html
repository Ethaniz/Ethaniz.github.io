<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Photo by Jon Tyson on Unsplash 前言CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发">
<meta name="keywords" content="Keras,LSTM">
<meta property="og:type" content="article">
<meta property="og:title" content="Keras中return_sequences和return_state有什么用？">
<meta property="og:url" content="https://ethaniz.github.io/posts/keras-return-sequences-return-state/index.html">
<meta property="og:site_name" content="机器拾趣">
<meta property="og:description" content="Photo by Jon Tyson on Unsplash 前言CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.loli.net/2019/09/19/ZTChqXdSywmJYPk.jpg">
<meta property="og:image" content="https://i.loli.net/2019/09/19/k4U3AgjmeE97x8y.jpg">
<meta property="og:image" content="https://i.loli.net/2019/09/19/4PMajuzCGpyShWt.png">
<meta property="og:updated_time" content="2019-09-25T10:22:23.634Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keras中return_sequences和return_state有什么用？">
<meta name="twitter:description" content="Photo by Jon Tyson on Unsplash 前言CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发">
<meta name="twitter:image" content="https://i.loli.net/2019/09/19/ZTChqXdSywmJYPk.jpg">
  <link rel="canonical" href="https://ethaniz.github.io/posts/keras-return-sequences-return-state/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Keras中return_sequences和return_state有什么用？ | 机器拾趣</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-147618128-2"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-147618128-2');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">机器拾趣</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">冰冷机器如何理解世事无常</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://ethaniz.github.io/posts/keras-return-sequences-return-state/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="异尘">
      <meta itemprop="description" content="现居成都，多年大数据与机器学习经验，目前任某互联网公司算法专家">
      <meta itemprop="image" content="https://i.loli.net/2019/09/26/LGwrSnIpZH8UFJM.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="机器拾趣">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Keras中return_sequences和return_state有什么用？

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-09-19 12:14:51" itemprop="dateCreated datePublished" datetime="2019-09-19T12:14:51+08:00">2019-09-19</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-09-25 18:22:23" itemprop="dateModified" datetime="2019-09-25T18:22:23+08:00">2019-09-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="https://i.loli.net/2019/09/19/ZTChqXdSywmJYPk.jpg" alt> Photo by Jon Tyson on Unsplash</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发展的。</p>
<p>今天，我们以LSTM为例，来谈一个RNN中的一个具体的问题。我们知道，在Keras的LSTM实现中，有两个参数<code>return_sequences</code>和<code>return_state</code>。这两个参数的实际意义是什么？在什么场景下会用到呢？</p>
<p>PS：<code>Keras</code>是我最喜爱的深度学习框架了，其API的设计非常精妙和优雅，François Chollet是不愧是大师中的大师。相比传统的<code>Tensorflow</code>和<code>PyTorch</code>，Keras的API才是真正的“Deep Learning for Human”。另外，看到<code>Tensorflow 2.0</code>也开始以<code>tf.keras</code>作为第一公民，我非常欣慰。关于我对这几个框架的理解，后面再以专题文章和大家分享。</p>
<a id="more"></a>

<hr>
<h2 id="LSTM介绍"><a href="#LSTM介绍" class="headerlink" title="LSTM介绍"></a>LSTM介绍</h2><blockquote>
<p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a> , and were refined and popularized by many people in following work.They work tremendously well on a large variety of problems, and are now widely used.</p>
</blockquote>
<p>LSTM是为了解决普通RNN网络在实际实践中出现的“梯度消失”等问题而出现的。这里我们略过里面的细节，重点看看单个LSTM cell的输入输出情况。<br><img src="https://i.loli.net/2019/09/19/k4U3AgjmeE97x8y.jpg" alt><br>从上图可以看出，单个LSTM cell其实有2个输出的，一个是h(t)，一个是c(t)。这里的h(t)称为hidden state，c(t)称为cell state。这个命名其实我认为是不太好的。熟悉全连接神经网络的同学，一定会把h(t)跟hidden layer相混淆。其实，这个h(t)才是LSTM的真正output，c(t)才是LSTM的内部”隐藏”状态。 </p>
<p>我们进一步把LSTM网络展开来看。每一个时间节点timestep，输入一个x(t)，cell里面的c(t)做一次更新，输出h(t)。紧接着下一个timestep，x(t+1)、h(t)和c(t)继续输入到cell，输出为h(t+1)和c(t+1)，如下图。<br><img src="https://i.loli.net/2019/09/19/4PMajuzCGpyShWt.png" alt><br>因此，Keras中的<code>return_sequences</code>和<code>return_state</code>，就是跟h(t)和c(t)相关。</p>
<hr>
<h2 id="Return-Sequences"><a href="#Return-Sequences" class="headerlink" title="Return Sequences"></a>Return Sequences</h2><p>接下来我们来点hands-on的代码，来具体看看这两个参数的作用。 </p>
<h3 id="实验一"><a href="#实验一" class="headerlink" title="实验一"></a>实验一</h3><p>试验代码中，<code>return_sequences</code>和<code>return_state</code>默认都是false，输入shape为(1,3,1)，表示1个batch，3个timestep，1个feature</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line">inputs1 = Input(shape=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">lstm1 = LSTM(<span class="number">1</span>)(inputs1)</span><br><span class="line">model = Model(inputs=inputs1, outputs=lstm1)</span><br><span class="line"><span class="comment"># define input data</span></span><br><span class="line">data = array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>]).reshape((<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># make and show prediction</span></span><br><span class="line">print(model.predict(data))</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">-0.0953151</span>]]</span><br></pre></td></tr></table></figure>

<p>表示在经历了3个time step的输入后，LSTM返回的hidden state，也就是上文中的h(t)。由于输出的是网络最后一个timestep的值，因此结果是一个标量。 </p>
<h3 id="实验二"><a href="#实验二" class="headerlink" title="实验二"></a>实验二</h3><p>我们加上参数<code>return_sequences=True</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm1 = LSTM(<span class="number">1</span>, return_sequences=<span class="literal">True</span>)(inputs1)</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">-0.02243521</span>]</span><br><span class="line">[<span class="number">-0.06210149</span>]</span><br><span class="line">[<span class="number">-0.11457888</span>]]]</span><br></pre></td></tr></table></figure>

<p>我们看到，输出了一个array，长度等于timestep，表示网络输出了每个timestep的h(t)。</p>
<p>总结一下，<code>return_sequences</code>即表示，LSTM的输出h(t)，是输出最后一个timestep的h(t)，还是把所有timestep的h(t)都输出出来。在实际应用中，关系到网络的应用场景是many-to-one还是many-to-many，非常重要。</p>
<hr>
<h2 id="Return-State"><a href="#Return-State" class="headerlink" title="Return State"></a>Return State</h2><h3 id="实验三"><a href="#实验三" class="headerlink" title="实验三"></a>实验三</h3><p>接下来我们继续实验<code>return_state</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm1, state_h, state_c = LSTM(<span class="number">1</span>, return_state=<span class="literal">True</span>)(inputs1)</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[array([[ <span class="number">0.10951342</span>]], dtype=float32),</span><br><span class="line"> array([[ <span class="number">0.10951342</span>]], dtype=float32),</span><br><span class="line"> array([[ <span class="number">0.24143776</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure>

<p>注意，输出是一个列表list，分别表示</p>
<ul>
<li>最后一个time step的hidden state</li>
<li>最后一个time step的hidden state（跟上面一样)</li>
<li>最后一个time step的cell state（注意就是上文中的c(t)） </li>
</ul>
<p>可以看出，<code>return_state</code>就是控制LSTM中的c(t)输出与否。 </p>
<h3 id="实验四"><a href="#实验四" class="headerlink" title="实验四"></a>实验四</h3><p>我们最后看看<code>return_sequences</code>和<code>return_state</code>全开的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm1, state_h, state_c = LSTM(<span class="number">1</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[array([[[<span class="number">-0.02145359</span>],</span><br><span class="line">         [<span class="number">-0.0540871</span> ],</span><br><span class="line">         [<span class="number">-0.09228823</span>]]], dtype=float32),</span><br><span class="line"> array([[<span class="number">-0.09228823</span>]], dtype=float32),</span><br><span class="line"> array([[<span class="number">-0.19803026</span>]], dtype=float32)]</span><br></pre></td></tr></table></figure>

<p>输出列表的意义其实跟上面实验三一致，只是第一个hidden state h(t)变成了所有timestep的，因此也是长度等于timestep的array。</p>
<hr>
<h2 id="Time-Distributed"><a href="#Time-Distributed" class="headerlink" title="Time Distributed"></a>Time Distributed</h2><p>最后再讲一讲<code>Keras</code>中的<code>TimeDistributed</code>。这个也是在RNN中非常常用但比较难理解的概念，原作者解释说</p>
<blockquote>
<p>TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor. </p>
</blockquote>
<p>其实它的主要用途在于Many-to-Many： 比如输入shape为(1, 5, 1)，输出shape为(1, 5, 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">3</span>, input_shape=(length, <span class="number">1</span>), return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(TimeDistributed(Dense(<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>

<p>根据上面解读，<code>return_sequences=True</code>，使得LSTM的输出为每个timestep的hidden state，shape为(1, 5, 3) </p>
<p>现在需要将这个(1 ,5, 3)的3D tensor变换为(1, 5, 1)的结果，需要3个Dense layer，分别作用于每个time step的输出。而使用了<code>TimeDistributed</code>后，则把一个相同的Dense layer去分别作用，可以使得网络更为紧凑，参数更少的作用。</p>
<p>如果是在many-to-one的情况，<code>return_sequence=False</code>，则LSTM的输出为最后一个time step的hidden state，shape为(1, 3)。此时加上一个Dense layer, 不用使用<code>TimeDistributed</code>，就可以将(1, 3)变换为(1, 1)。</p>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要通过一些实际的代码案例，解释了<code>Keras</code>的LSTM API中常见的两个参数<code>return_sequence</code>和<code>return_state</code>的原理及作用，在<code>Tensorflow</code>及<code>PyTorch</code>，也有相通的，希望能够帮助大家加深对RNN的理解。</p>

    </div>

    
    
    
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center;">
  <img id="wechat_subscriber_qcode" src="/uploads/wechat-qcode.jpg" alt="异尘 wechat" style="width: 200px; max-width: 100%;">
  <div>欢迎您扫一扫上面的微信公众号，订阅我的博客</div>
</div>

      
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/Keras/" rel="tag"># Keras</a>
            
              <a href="/tags/LSTM/" rel="tag"># LSTM</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/posts/numpy-naive-bayes/" rel="next" title="numpy“手撕”朴素贝叶斯">
                  <i class="fa fa-chevron-left"></i> numpy“手撕”朴素贝叶斯
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/posts/numpy-pca-svd/" rel="prev" title="使用numpy来理解PCA和SVD">
                  使用numpy来理解PCA和SVD <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM介绍"><span class="nav-number">2.</span> <span class="nav-text">LSTM介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Return-Sequences"><span class="nav-number">3.</span> <span class="nav-text">Return Sequences</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实验一"><span class="nav-number">3.1.</span> <span class="nav-text">实验一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验二"><span class="nav-number">3.2.</span> <span class="nav-text">实验二</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Return-State"><span class="nav-number">4.</span> <span class="nav-text">Return State</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实验三"><span class="nav-number">4.1.</span> <span class="nav-text">实验三</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验四"><span class="nav-number">4.2.</span> <span class="nav-text">实验四</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Time-Distributed"><span class="nav-number">5.</span> <span class="nav-text">Time Distributed</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://i.loli.net/2019/09/26/LGwrSnIpZH8UFJM.jpg"
      alt="异尘">
  <p class="site-author-name" itemprop="name">异尘</p>
  <div class="site-description" itemprop="description">现居成都，多年大数据与机器学习经验，目前任某互联网公司算法专家</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/yi-chen-25-73" title="知乎 &rarr; https://www.zhihu.com/people/yi-chen-25-73" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">异尘</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'cb5f02dcae7fb041a28a',
      clientSecret: 'af1050d07337b233d629af013bda2856c23932a5',
      repo: 'ethaniz.github.io',
      owner: 'ethaniz',
      admin: ['ethaniz'],
      id: 'b22d085a73d5e9db2543bf2b6df2abab',
        language: window.navigator.language || window.navigator.userLanguage,
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

</body>
</html>
